<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 2: GPU vs CPU Architecture | Create Your Own GPU</title>
    <link rel="stylesheet" href="../styles.css" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
    <style>
        .chapter-content { max-width: 800px; margin: 0 auto; padding: 40px 20px; }
        .chapter-nav { display: flex; justify-content: space-between; margin: 40px 0; padding: 20px; background: var(--panel); border-radius: var(--radius); }
        .figure { margin: 30px 0; padding: 20px; background: #f9f9ff; border-radius: 12px; }
        .figure-caption { font-style: italic; color: var(--muted); margin-top: 10px; text-align: center; }
        .key-takeaway { background: #e8f4fd; border-left: 4px solid #2196f3; padding: 16px; margin: 24px 0; }
        .exercise { background: #fff3e0; border-left: 4px solid #ff9800; padding: 16px; margin: 24px 0; }
        .engineer-note { background: #f3e5f5; border-left: 4px solid #9c27b0; padding: 16px; margin: 24px 0; }
        .mermaid { background: transparent; text-align: center; }
        .comparison-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        .comparison-table th, .comparison-table td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        .comparison-table th { background: #f1f5f9; }
        .comparison-table tr:nth-child(even) { background: #f9fafb; }
        .analogy-box { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }
        .analogy-card { padding: 20px; border-radius: 12px; }
        .analogy-card.cpu { background: linear-gradient(145deg, #eef2ff, #e0e7ff); border: 2px solid #6366f1; }
        .analogy-card.gpu { background: linear-gradient(145deg, #ecfdf5, #d1fae5); border: 2px solid #10b981; }
        .analogy-card h4 { margin-top: 0; }
        .budget-bar { display: flex; height: 30px; border-radius: 4px; overflow: hidden; margin: 10px 0; }
        .budget-bar span { display: flex; align-items: center; justify-content: center; color: white; font-size: 11px; font-weight: bold; }
    </style>
</head>
<body>
    <div class="chapter-content">
        <div class="chapter-nav">
            <a href="chapter-01.html">‚Üê Previous: The Big Picture</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-03.html">Next: Parallel Execution ‚Üí</a>
        </div>

        <h1>Chapter 2: GPU vs CPU Architecture</h1>
        
        <div class="meta" style="color: var(--muted); margin: 20px 0;">
            <span>Part I: GPU Fundamentals</span> ‚Ä¢ 
            <span>Reading time: ~45 min</span>
        </div>

        <h2>Introduction</h2>
        <p>
            Now that we understand the "what" of GPUs, let's dig into the "how" ‚Äî specifically, how GPU 
            architecture fundamentally differs from CPU architecture. These aren't just different chips; 
            they're designed for completely different purposes with completely different philosophies.
        </p>

        <h2>2.1 The Chef Analogy</h2>

        <div class="analogy-box">
            <div class="analogy-card cpu">
                <h4>üßë‚Äçüç≥ CPU = Master Chef</h4>
                <ul>
                    <li>One highly skilled chef</li>
                    <li>Can handle any recipe (general purpose)</li>
                    <li>Remembers customer preferences (speculation)</li>
                    <li>Adapts on the fly to special requests</li>
                    <li>Has all ingredients within arm's reach (large cache)</li>
                </ul>
            </div>
            <div class="analogy-card gpu">
                <h4>üë®‚Äçüç≥üë®‚Äçüç≥üë®‚Äçüç≥ GPU = 100 Line Cooks</h4>
                <ul>
                    <li>Many simple cooks, each doing one task</li>
                    <li>Excellent at making 100 identical dishes</li>
                    <li>Follow exact instructions only</li>
                    <li>Struggle with custom orders</li>
                    <li>Share a central ingredient storage (DRAM)</li>
                </ul>
            </div>
        </div>

        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph CPU["CPU: Master Chef"]
        C1["Complex<br/>Order"] --> C2["Prepare<br/>Ingredients"]
        C2 --> C3["Cook<br/>Dish"]
        C3 --> C4["Plate &<br/>Garnish"]
        C4 --> C5["Serve"]
    end
    
    subgraph GPU["GPU: Line Cook Army"]
        direction TB
        O1["Order 1"] --> LC1["Cook 1"]
        O2["Order 2"] --> LC2["Cook 2"]
        O3["Order 3"] --> LC3["Cook 3"]
        O4["Order 4"] --> LC4["Cook 4"]
        O5["..."] --> LC5["..."]
        O6["Order 100"] --> LC6["Cook 100"]
    end
    
    style CPU fill:#eef2ff,stroke:#6366f1,stroke-width:2px
    style GPU fill:#ecfdf5,stroke:#10b981,stroke-width:2px
            </div>
            <div class="figure-caption">Figure 2.1: CPU processes orders sequentially; GPU processes many orders in parallel</div>
        </div>

        <h2>2.2 Transistor Budget: How Chips Spend Their Billions</h2>

        <p>
            A modern CPU like AMD's Ryzen 9 7950X has about <strong>13 billion transistors</strong>. 
            A modern GPU like NVIDIA's H100 has about <strong>80 billion transistors</strong>. 
            But it's not just about quantity ‚Äî it's about allocation.
        </p>

        <h3>CPU Transistor Budget</h3>
        <div class="budget-bar">
            <span style="width: 50%; background: #3b82f6;">Caches 50%</span>
            <span style="width: 25%; background: #8b5cf6;">Control 25%</span>
            <span style="width: 15%; background: #ec4899;">ALU/FPU 15%</span>
            <span style="width: 10%; background: #f97316;">Other 10%</span>
        </div>

        <table class="comparison-table">
            <tr>
                <th>Component</th>
                <th>% Budget</th>
                <th>Purpose</th>
            </tr>
            <tr>
                <td>L1/L2/L3 Caches</td>
                <td>~50%</td>
                <td>Keep frequently-accessed data close to cores</td>
            </tr>
            <tr>
                <td>Branch Prediction</td>
                <td>~10%</td>
                <td>Guess which way branches will go</td>
            </tr>
            <tr>
                <td>Out-of-Order Engine</td>
                <td>~15%</td>
                <td>Reorder instructions for maximum ILP</td>
            </tr>
            <tr>
                <td>ALU/FPU Units</td>
                <td>~15%</td>
                <td>Actually do math</td>
            </tr>
            <tr>
                <td>Other (I/O, etc.)</td>
                <td>~10%</td>
                <td>Memory controllers, interconnects</td>
            </tr>
        </table>

        <h3>GPU Transistor Budget</h3>
        <div class="budget-bar">
            <span style="width: 60%; background: #10b981;">ALUs 60%</span>
            <span style="width: 20%; background: #6366f1;">Registers 20%</span>
            <span style="width: 12%; background: #f59e0b;">Cache 12%</span>
            <span style="width: 8%; background: #8b5cf6;">Ctrl 8%</span>
        </div>

        <table class="comparison-table">
            <tr>
                <th>Component</th>
                <th>% Budget</th>
                <th>Purpose</th>
            </tr>
            <tr>
                <td>ALU/FPU Arrays</td>
                <td>~55-60%</td>
                <td>Massive compute capability</td>
            </tr>
            <tr>
                <td>Register Files</td>
                <td>~15-20%</td>
                <td>Per-thread state storage</td>
            </tr>
            <tr>
                <td>Caches</td>
                <td>~10-15%</td>
                <td>Smaller caches, rely on parallelism to hide latency</td>
            </tr>
            <tr>
                <td>Control Logic</td>
                <td>~5-10%</td>
                <td>Simple schedulers, minimal prediction</td>
            </tr>
        </table>

        <div class="figure">
            <div class="mermaid">
block-beta
    columns 2
    
    block:cpuBlock["CPU Die Layout"]:1
        columns 1
        space
        L3["L3 Cache (Huge)"]
        space
        block:cores
            columns 4
            C1["Core"] C2["Core"] C3["Core"] C4["Core"]
        end
        block:cores2
            columns 4
            C5["Core"] C6["Core"] C7["Core"] C8["Core"]
        end
        space
        MC["Memory Controller + I/O"]
    end
    
    block:gpuBlock["GPU Die Layout"]:1
        columns 8
        SM1["SM"] SM2["SM"] SM3["SM"] SM4["SM"] SM5["SM"] SM6["SM"] SM7["SM"] SM8["SM"]
        SM9["SM"] SM10["SM"] SM11["SM"] SM12["SM"] SM13["SM"] SM14["SM"] SM15["SM"] SM16["SM"]
        SM17["SM"] SM18["SM"] SM19["SM"] SM20["SM"] SM21["SM"] SM22["SM"] SM23["SM"] SM24["SM"]
        SM25["SM"] SM26["SM"] SM27["SM"] SM28["SM"] SM29["SM"] SM30["SM"] SM31["SM"] SM32["SM"]
        L2A["L2 Cache"]:4 L2B["L2 Cache"]:4
        MCA["HBM"]:2 MCB["HBM"]:2 MCC["HBM"]:2 MCD["HBM"]:2
    end
    
    style cpuBlock fill:#eef2ff,stroke:#6366f1
    style gpuBlock fill:#ecfdf5,stroke:#10b981
            </div>
            <div class="figure-caption">Figure 2.2: CPU vs GPU die layout ‚Äî CPUs dominated by cache, GPUs by compute units</div>
        </div>

        <div class="key-takeaway">
            <strong>Key Insight:</strong> CPUs spend transistors on making one thing fast. GPUs spend 
            transistors on doing many things at once. This is the fundamental tradeoff.
        </div>

        <h2>2.3 Memory Hierarchy Deep Dive</h2>

        <p>
            Memory is the bottleneck in modern computing. Both CPUs and GPUs fight this, but in different ways.
        </p>

        <h3>CPU Memory Strategy: Hide Latency with Caches</h3>

        <p>
            CPUs assume you'll access the same data repeatedly (temporal locality) and nearby data 
            (spatial locality). They build massive cache hierarchies:
        </p>

        <ul>
            <li><strong>L1 cache:</strong> 32-64 KB per core, 4 cycle latency, split into data and instruction</li>
            <li><strong>L2 cache:</strong> 256 KB - 1 MB per core, ~12 cycle latency</li>
            <li><strong>L3 cache:</strong> 8-64 MB shared, ~40 cycle latency</li>
            <li><strong>Main Memory (DRAM):</strong> 16-128 GB, ~200 cycle latency</li>
        </ul>

        <h3>GPU Memory Strategy: Hide Latency with Threads</h3>

        <p>
            GPUs assume data won't fit in cache because you're processing massive datasets. Instead, they use 
            <strong>latency hiding</strong> through massive parallelism:
        </p>

        <ul>
            <li><strong>Registers:</strong> Thousands per SM, 1 cycle latency</li>
            <li><strong>Shared Memory/L1:</strong> 64-128 KB per SM, ~20 cycle latency</li>
            <li><strong>L2 cache:</strong> 40-80 MB shared across GPU, ~200 cycle latency</li>
            <li><strong>Global Memory (HBM/GDDR):</strong> 16-80 GB, ~400 cycle latency</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph CPU_MEM["CPU Memory Hierarchy"]
        direction TB
        R1["Registers<br/>~1 cycle"] --> L1["L1 Cache<br/>32-64KB, ~4 cycles"]
        L1 --> L2["L2 Cache<br/>256KB-1MB, ~12 cycles"]
        L2 --> L3["L3 Cache<br/>8-64MB, ~40 cycles"]
        L3 --> DRAM1["DRAM<br/>16-128GB, ~200 cycles"]
    end
    
    subgraph GPU_MEM["GPU Memory Hierarchy"]
        direction TB
        R2["Registers<br/>256KB/SM, 1 cycle"] --> SMEM["Shared Memory/L1<br/>64-128KB/SM, ~20 cycles"]
        SMEM --> GL2["L2 Cache<br/>40-80MB, ~200 cycles"]
        GL2 --> HBM["HBM/GDDR<br/>16-80GB, ~400 cycles"]
    end
    
    style CPU_MEM fill:#eef2ff,stroke:#6366f1
    style GPU_MEM fill:#ecfdf5,stroke:#10b981
            </div>
            <div class="figure-caption">Figure 2.3: Memory hierarchy comparison ‚Äî CPUs have more cache levels, GPUs have larger register files</div>
        </div>

        <div class="engineer-note">
            <strong>üîß Engineering Reality:</strong> When a warp (group of threads) hits a memory access, 
            the GPU doesn't wait. It switches to another ready warp. By the time it cycles through all warps, 
            the memory is back. This is called <strong>latency hiding through parallelism</strong>.
        </div>

        <div class="key-takeaway">
            <strong>Key Insight:</strong> CPUs hide latency with caches. GPUs hide latency with threads. 
            This fundamental difference shapes everything.
        </div>

        <h2>2.4 Control Flow: Speculation vs Simplicity</h2>

        <h3>CPU: The Oracle</h3>
        <p>
            CPUs run unpredictable code with lots of branches (if/else, loops, function calls). To go fast, 
            they <em>predict the future</em>:
        </p>

        <ul>
            <li><strong>Branch prediction:</strong> Guess which way a branch will go and speculatively execute</li>
            <li><strong>Out-of-order execution:</strong> Reorder instructions to maximize throughput</li>
            <li><strong>Speculative execution:</strong> Execute code before knowing if it's needed</li>
            <li><strong>Register renaming:</strong> Eliminate false dependencies</li>
        </ul>

        <h3>GPU: The Conductor</h3>
        <p>
            GPUs assume you're running the same code (kernel) across many data points. They use simple control:
        </p>

        <ul>
            <li><strong>SIMT execution:</strong> All threads in a warp run the same instruction</li>
            <li><strong>No branch prediction:</strong> Branches cause divergence, which is handled explicitly</li>
            <li><strong>In-order execution:</strong> Instructions execute in program order (mostly)</li>
            <li><strong>Simple scheduler:</strong> Round-robin or scoreboard-based, no speculation</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph CPU_BRANCH["CPU: Branch Prediction"]
        direction TB
        B1["if (x > 0)?"] --> PRED["Predict: likely TRUE"]
        PRED --> SPEC["Speculatively execute A()"]
        SPEC --> CHECK{"Was prediction<br/>correct?"}
        CHECK -->|Yes| COMMIT["Commit results"]
        CHECK -->|No| FLUSH["Flush pipeline,<br/>re-execute B()"]
    end
    
    subgraph GPU_BRANCH["GPU: Branch Divergence"]
        direction TB
        B2["if (threadIdx % 2)?"] --> DIV["Warp Diverges"]
        DIV --> MASK1["Threads 0,2,4... execute A()"]
        MASK1 --> MASK2["Threads 1,3,5... execute B()"]
        MASK2 --> RECONVERGE["Reconverge at endif"]
    end
    
    style CPU_BRANCH fill:#eef2ff,stroke:#6366f1
    style GPU_BRANCH fill:#ecfdf5,stroke:#10b981
            </div>
            <div class="figure-caption">Figure 2.4: CPU speculates and may need to rollback; GPU executes both paths with masking</div>
        </div>

        <h2>2.5 Execution Pipelines</h2>

        <h3>CPU: Deep and Complex</h3>
        <p>
            A typical CPU has a 12-20 stage pipeline:
        </p>

        <ol>
            <li>Fetch 1, Fetch 2 (get instruction from cache)</li>
            <li>Decode 1, Decode 2, Decode 3 (complex x86 decode)</li>
            <li>Rename (register renaming)</li>
            <li>Dispatch (send to execution units)</li>
            <li>Execute 1, Execute 2, ... (depends on operation)</li>
            <li>Write-back</li>
            <li>Commit (make results visible)</li>
        </ol>

        <h3>GPU: Wide and Simple</h3>
        <p>
            A typical GPU core has a 6-10 stage pipeline:
        </p>

        <ol>
            <li>Fetch (per-warp instruction fetch)</li>
            <li>Decode (simple fixed-width instruction)</li>
            <li>Issue (select ready warp)</li>
            <li>Read (register file access)</li>
            <li>Execute (ALU/FPU/LSU)</li>
            <li>Write (register file write)</li>
        </ol>

        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph CPU_PIPE["CPU Pipeline (14+ stages)"]
        direction LR
        F1["Fetch"] --> F2["Fetch2"] --> D1["Decode"] --> D2["Decode2"] --> D3["Decode3"]
        D3 --> RN["Rename"] --> DIS["Dispatch"] --> ISS["Issue"]
        ISS --> EX1["Execute"] --> EX2["Exec2"] --> WB["Write"] --> COM["Commit"]
    end
    
    subgraph GPU_PIPE["GPU Pipeline (6 stages)"]
        direction LR
        GF["Fetch"] --> GD["Decode"] --> GI["Issue"] --> GR["Read Regs"] --> GE["Execute"] --> GW["Write Regs"]
    end
    
    style CPU_PIPE fill:#eef2ff,stroke:#6366f1
    style GPU_PIPE fill:#ecfdf5,stroke:#10b981
            </div>
            <div class="figure-caption">Figure 2.5: CPU pipelines are deep for high frequency; GPU pipelines are short for simplicity</div>
        </div>

        <p>
            Deep pipelines allow high clock speeds (5+ GHz) but require complex hazard handling. 
            Simpler pipelines mean lower clock speeds (1.5-2 GHz) but massive replication is feasible.
        </p>

        <h2>2.6 Power and Thermal Considerations</h2>

        <div class="figure">
            <div class="mermaid">
xychart-beta
    title "Power Consumption vs Performance"
    x-axis ["Low Power", "Medium", "High", "Max"]
    y-axis "Performance (relative)" 0 --> 100
    bar [20, 50, 80, 100]
    line [15, 40, 70, 85]
            </div>
            <div class="figure-caption">Figure 2.6: Power efficiency curves ‚Äî diminishing returns at high power</div>
        </div>

        <h3>CPU: High Frequency, High Power Density</h3>
        <ul>
            <li>Clock speeds: 3-5 GHz (base), up to 5.5 GHz (turbo)</li>
            <li>Power: 65-250W for desktop/server CPUs</li>
            <li>Power density: Very high (requires sophisticated cooling)</li>
            <li>Voltage: Dynamic voltage and frequency scaling (DVFS)</li>
        </ul>

        <h3>GPU: Lower Frequency, Spread Across Area</h3>
        <ul>
            <li>Clock speeds: 1.5-2.5 GHz (more consistent)</li>
            <li>Power: 150-600W for high-end GPUs</li>
            <li>Power density: Lower per unit area (large die spreads heat)</li>
            <li>Thermal throttling: Reduces clocks if too hot</li>
        </ul>

        <div class="key-takeaway">
            <strong>Engineering Reality:</strong> Power is the limiting factor for both architectures. Modern 
            chips are "power-limited" not "transistor-limited" ‚Äî you can't turn on all transistors at max 
            frequency without melting the chip.
        </div>

        <h2>2.7 When to Use CPU vs GPU</h2>

        <div class="figure">
            <div class="mermaid">
quadrantChart
    title Workload Classification
    x-axis Sequential --> Parallel
    y-axis Simple --> Complex
    quadrant-1 GPU Domain
    quadrant-2 CPU for Setup, GPU for Work
    quadrant-3 CPU Domain
    quadrant-4 Either Works
    "Neural Network Training": [0.9, 0.3]
    "Video Encoding": [0.8, 0.4]
    "Web Server": [0.2, 0.8]
    "Database Query": [0.3, 0.7]
    "Matrix Multiply": [0.95, 0.2]
    "Compiler": [0.15, 0.9]
    "Physics Sim": [0.7, 0.5]
            </div>
            <div class="figure-caption">Figure 2.7: Workload classification ‚Äî parallel simple tasks favor GPU; complex sequential favor CPU</div>
        </div>

        <h3>Use a CPU when:</h3>
        <ul>
            <li>You have sequential, unpredictable code with lots of branches</li>
            <li>You need low latency for individual operations</li>
            <li>Data fits in cache and exhibits locality</li>
            <li>You need general-purpose computing flexibility</li>
            <li>You're running diverse workloads that change frequently</li>
        </ul>

        <h3>Use a GPU when:</h3>
        <ul>
            <li>You have data-parallel work (same operation on many data points)</li>
            <li>You need high throughput (total work done per second)</li>
            <li>Data is too large to fit in cache</li>
            <li>You can tolerate higher latency for individual operations</li>
            <li>The workload is predictable and sustained</li>
        </ul>

        <h3>Decision Examples:</h3>

        <table class="comparison-table">
            <tr>
                <th>Task</th>
                <th>Best Choice</th>
                <th>Reason</th>
            </tr>
            <tr>
                <td>Web server handling requests</td>
                <td>CPU</td>
                <td>Unpredictable, low-latency required</td>
            </tr>
            <tr>
                <td>Training a neural network</td>
                <td>GPU</td>
                <td>Matrix multiply is highly parallel</td>
            </tr>
            <tr>
                <td>Database query processing</td>
                <td>CPU (mostly)</td>
                <td>Irregular access patterns, branches</td>
            </tr>
            <tr>
                <td>Video encoding</td>
                <td>GPU</td>
                <td>Pixel processing is parallel</td>
            </tr>
            <tr>
                <td>Compiling code</td>
                <td>CPU</td>
                <td>Complex, sequential logic</td>
            </tr>
            <tr>
                <td>Scientific simulation (weather)</td>
                <td>GPU</td>
                <td>Grid-based computation, parallel</td>
            </tr>
        </table>

        <h2>2.8 Hybrid Approaches: Heterogeneous Computing</h2>

        <p>
            Modern systems often use both! This is called <strong>heterogeneous computing</strong>:
        </p>

        <div class="figure">
            <div class="mermaid">
sequenceDiagram
    participant App as Application
    participant CPU as CPU
    participant GPU as GPU
    participant MEM as GPU Memory
    
    App->>CPU: Start program
    CPU->>CPU: Initialize, parse input
    CPU->>MEM: Allocate GPU memory
    CPU->>MEM: Copy input data
    CPU->>GPU: Launch kernel
    GPU->>GPU: Execute parallel work
    GPU->>MEM: Store results
    CPU->>MEM: Copy results back
    CPU->>App: Return output
            </div>
            <div class="figure-caption">Figure 2.8: Typical CPU-GPU workflow in heterogeneous computing</div>
        </div>

        <ul>
            <li>CPU handles control flow, setup, and sequential parts</li>
            <li>GPU handles data-parallel compute-intensive parts</li>
            <li>Data transfers between CPU and GPU memory as needed</li>
        </ul>

        <p>
            Frameworks like CUDA, OpenCL, and oneAPI make this programming model accessible.
        </p>

        <div class="exercise">
            <strong>Exercise 2.1:</strong> Calculate: If a CPU has 4 cores and a GPU has 80 SMs with 64 CUDA 
            cores each, how many execution units does each have? What does this tell you about parallelism?
        </div>

        <div class="exercise">
            <strong>Exercise 2.2:</strong> Research the die photo of an Intel or AMD CPU and an NVIDIA GPU. 
            Identify the caches, execution units, and memory controllers. Estimate the area percentages.
        </div>

        <div class="exercise">
            <strong>Exercise 2.3:</strong> Write a simple program (e.g., vector addition) that runs on both 
            CPU and GPU. Measure performance for different data sizes. At what size does GPU win?
        </div>

        <h2>2.9 Chapter Summary</h2>

        <p>
            In this chapter, we explored:
        </p>
        <ul>
            <li>Transistor budget allocation: CPUs spend on control/cache, GPUs on compute</li>
            <li>Memory hierarchies: CPUs hide latency with caches, GPUs with threads</li>
            <li>Control flow: CPUs predict and speculate, GPUs keep it simple</li>
            <li>Execution pipelines: CPUs deep and complex, GPUs wide and simple</li>
            <li>Power and thermal tradeoffs</li>
            <li>When to use each architecture</li>
        </ul>

        <div class="key-takeaway">
            <strong>Next Steps:</strong> In Chapter 3, we'll dive into parallel execution models ‚Äî SIMD vs SIMT, 
            threads vs warps, and how kernels actually run on GPU hardware.
        </div>

        <h2>Further Reading</h2>
        <ul>
            <li>"Modern Processor Design" by Shen & Lipasti ‚Äî excellent CPU architecture coverage</li>
            <li>NVIDIA Ampere Architecture Whitepaper ‚Äî detailed GPU architecture</li>
            <li>"Computer Architecture: A Quantitative Approach" by Hennessy & Patterson ‚Äî Chapter 4</li>
            <li>Papers: "A Case for GPGPU Spatial Multitasking" (HPCA 2012)</li>
        </ul>

        <div class="chapter-nav">
            <a href="chapter-01.html">‚Üê Chapter 1: Big Picture</a>
            <a href="chapter-03.html">Next: Parallel Execution ‚Üí</a>
        </div>
    </div>

    <script>
        mermaid.initialize({ 
            startOnLoad: true, 
            theme: 'base',
            themeVariables: {
                primaryColor: '#6366f1',
                primaryTextColor: '#4f46e5',
                primaryBorderColor: '#4f46e5',
                lineColor: '#64748b',
                secondaryColor: '#10b981',
                tertiaryColor: '#f1f5f9'
            }
        });
    </script>
    <script src="../navigation.js"></script>
</body>
</html>
