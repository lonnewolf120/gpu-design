<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 6: Pipeline Design and Control Flow | Create Your Own GPU</title>
    <link rel="stylesheet" href="../styles.css" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
    <style>
        .chapter-content { max-width: 900px; margin: 0 auto; padding: 40px 20px; }
        .chapter-nav { display: flex; justify-content: space-between; margin: 40px 0; padding: 20px; background: var(--panel); border-radius: var(--radius); }
        .figure { margin: 30px 0; padding: 20px; background: #f9f9ff; border-radius: 12px; overflow-x: auto; }
        .figure-caption { font-style: italic; color: var(--muted); margin: 10px 0 0 0; text-align: center; }
        .key-takeaway { background: #e8f4fd; border-left: 4px solid #2196f3; padding: 16px; margin: 24px 0; }
        .exercise { background: #fff3e0; border-left: 4px solid #ff9800; padding: 16px; margin: 24px 0; }
        .engineer-note { background: #f3e5f5; border-left: 4px solid #9c27b0; padding: 16px; margin: 24px 0; }
        .code-block { background: #1e1e1e; color: #d4d4d4; padding: 20px; border-radius: 8px; overflow-x: auto; margin: 20px 0; font-family: monospace; font-size: 13px; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background: #f5f5f5; font-weight: bold; }
        .mermaid { background: white; padding: 20px; border-radius: 8px; border: 1px solid #ddd; margin: 20px 0; }
        pre { font-size: 12px; line-height: 1.4; }
    </style>
</head>
<body>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'base', themeVariables: { primaryColor: '#e3f2fd', primaryTextColor: '#1565c0', primaryBorderColor: '#90caf9', lineColor: '#64b5f6', secondaryColor: '#f3e5f5', tertiaryColor: '#fff3e0' } });
    </script>

    <div class="chapter-content">
        <div class="chapter-nav">
            <a href="chapter-05.html">← Previous: ISA Design</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-07.html">Next: Memory Hierarchy →</a>
        </div>

        <h1>Chapter 6: Pipeline Design and Control Flow</h1>
        
        <div class="meta" style="color: var(--muted); margin: 20px 0;">
            <span>Part II: Execution & Scheduling</span> • 
            <span>Reading time: ~70 min</span>
        </div>

        <h2>Introduction</h2>
        <p>
            In Chapter 4, we looked at GPU components in isolation. In Chapter 5, we designed an ISA. 
            Now we bring them together: the <strong>pipeline</strong> is the orchestration that turns 
            individual components into a functional system.
        </p>

        <p>
            A pipeline is like an assembly line in a factory. Instead of completing one instruction 
            from start to finish before starting the next, we break each instruction into stages. While one 
            instruction is in the "Execute" stage, another is in "Memory" stage, and a third is in "Writeback". 
            This parallelism increases throughput dramatically.
        </p>

        <h2>6.1 The Basic Fetch-Execute Cycle</h2>

        <p>
            Before pipelining, execution is simple but slow:
        </p>

        <div class="mermaid">
            %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffccbc'}}}%%
            gantt
                title Unpipelined Execution (25 cycles for 5 instructions)
                dateFormat X
                axisFormat %s
                section Instr 1
                    Fetch     :0, 1
                    Decode    :1, 2
                    Execute   :2, 3
                    Memory    :3, 4
                    Writeback :4, 5
                section Instr 2
                    Fetch     :5, 6
                    Decode    :6, 7
                    Execute   :7, 8
                    Memory    :8, 9
                    Writeback :9, 10
                section Instr 3
                    Fetch     :10, 11
                    Decode    :11, 12
                    Execute   :12, 13
                    Memory    :13, 14
                    Writeback :14, 15
        </div>
        <div class="figure-caption">Figure 6.1: Unpipelined execution — each instruction completes before the next starts</div>

        <p>
            With pipelining, all stages work simultaneously:
        </p>

        <div class="mermaid">
            %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#c8e6c9'}}}%%
            gantt
                title Pipelined Execution (9 cycles for 5 instructions)
                dateFormat X
                axisFormat %s
                section Instr 1
                    Fetch     :0, 1
                    Decode    :1, 2
                    Execute   :2, 3
                    Memory    :3, 4
                    Writeback :4, 5
                section Instr 2
                    Fetch     :1, 2
                    Decode    :2, 3
                    Execute   :3, 4
                    Memory    :4, 5
                    Writeback :5, 6
                section Instr 3
                    Fetch     :2, 3
                    Decode    :3, 4
                    Execute   :4, 5
                    Memory    :5, 6
                    Writeback :6, 7
                section Instr 4
                    Fetch     :3, 4
                    Decode    :4, 5
                    Execute   :5, 6
                    Memory    :6, 7
                    Writeback :7, 8
                section Instr 5
                    Fetch     :4, 5
                    Decode    :5, 6
                    Execute   :6, 7
                    Memory    :7, 8
                    Writeback :8, 9
        </div>
        <div class="figure-caption">Figure 6.2: Pipelined execution — 5 instructions in 9 cycles (vs 25 unpipelined)!</div>

        <h2>6.2 The 7-Stage GPU Pipeline</h2>

        <h3>Stage Breakdown</h3>

        <table>
            <thead>
                <tr>
                    <th>Stage</th>
                    <th>Abbrev</th>
                    <th>Description</th>
                    <th>Duration</th>
                </tr>
            </thead>
            <tbody>
                <tr><td><strong>Fetch</strong></td><td>F</td><td>Read instruction from I-cache using PC</td><td>1 cycle (cache hit)</td></tr>
                <tr><td><strong>Decode</strong></td><td>D</td><td>Parse opcode, identify registers, generate control signals</td><td>1 cycle</td></tr>
                <tr><td><strong>Issue</strong></td><td>I</td><td>Check scoreboard, allocate resources, schedule execution</td><td>1 cycle</td></tr>
                <tr><td><strong>Read</strong></td><td>R</td><td>Read operands from register file</td><td>1 cycle</td></tr>
                <tr><td><strong>Execute</strong></td><td>E</td><td>ALU operation, address computation, branch evaluation</td><td>1 cycle (may be multiple for MUL)</td></tr>
                <tr><td><strong>Memory</strong></td><td>M</td><td>Cache access (L1/L2), or main memory request</td><td>1 cycle (cache) to 200+ (DRAM)</td></tr>
                <tr><td><strong>Writeback</strong></td><td>W</td><td>Write result back to register file</td><td>1 cycle</td></tr>
            </tbody>
        </table>

        <h3>Detailed Pipeline Diagram</h3>

        <div class="mermaid">
            flowchart TB
                subgraph FETCH["STAGE 1: FETCH"]
                    PC["Program Counter<br/>Warp[0..31].pc"]
                    IC["I-Cache<br/>(8-32 KB)"]
                    FR["F Stage Register<br/>(Instruction + PC)"]
                    PC --> IC --> FR
                end
                
                subgraph DECODE["STAGE 2: DECODE"]
                    DEC["Decoder Module<br/>• Extract opcode[15:12]<br/>• Extract dst, src1, src2<br/>• Generate control signals"]
                    DR["D Stage Register<br/>(Control Signals)"]
                    DEC --> DR
                end
                
                subgraph ISSUE["STAGE 3: ISSUE"]
                    SB["Scoreboard<br/>• Check dependencies<br/>• Mark pending writes"]
                    WS["Warp Scheduler<br/>• Select warp<br/>• Check readiness"]
                end
                
                subgraph READ["STAGE 4: READ"]
                    RF["Register File<br/>• Read src1, src2<br/>• 32 threads parallel"]
                    RR["R Stage Register<br/>(Operand Values)"]
                    RF --> RR
                end
                
                subgraph EXECUTE["STAGE 5: EXECUTE"]
                    ALU["ALU Array (32-wide)<br/>Same op, different data"]
                    BEU["Branch Evaluation"]
                    ADDR["Address Computation"]
                end
                
                subgraph MEMORY["STAGE 6: MEMORY"]
                    LSU["Load/Store Unit<br/>• Coalesce addresses"]
                    L1["L1 Cache (20-40 cy)"]
                    L2["L2 Cache (100-200 cy)"]
                    DRAM["DRAM (200-400 cy)"]
                    LSU --> L1 --> L2 --> DRAM
                end
                
                subgraph WRITEBACK["STAGE 7: WRITEBACK"]
                    DIST["Result Distributor<br/>• Write to dest register<br/>• Clear pending bit"]
                    WRF["Register File<br/>(Writeback Port)"]
                    DIST --> WRF
                end
                
                FETCH --> DECODE --> ISSUE --> READ --> EXECUTE --> MEMORY --> WRITEBACK
                
                style FETCH fill:#e3f2fd
                style DECODE fill:#bbdefb
                style ISSUE fill:#90caf9
                style READ fill:#81d4fa
                style EXECUTE fill:#4fc3f7
                style MEMORY fill:#29b6f6
                style WRITEBACK fill:#03a9f4
        </div>
        <div class="figure-caption">Figure 6.3: The 7-stage GPU pipeline architecture</div>

        <h2>6.3 Pipeline Hazards</h2>

        <p>
            Hazards are situations where instructions cannot proceed normally. There are three types:
        </p>

        <h3>Data Hazards (Read-After-Write)</h3>

        <p>An instruction needs data that hasn't been written yet:</p>

        <div class="code-block">
<pre>ADD R1, R2, R3    // Instruction 1: R1 = R2 + R3
SUB R4, R1, R5    // Instruction 2: R4 = R1 - R5  (needs R1!)
                  // ^^^ HAZARD! R1 won't be available until cycle 5</pre>
        </div>

        <div class="mermaid">
            flowchart LR
                subgraph Problem["DATA HAZARD"]
                    ADD["ADD R1,R2,R3<br/>Writes R1 in cycle 5"]
                    SUB["SUB R4,R1,R5<br/>Needs R1 in cycle 3"]
                    ADD -.->|"R1 not ready!"| SUB
                end
                
                subgraph Solution1["SOLUTION 1: STALL"]
                    S1["Insert bubbles<br/>Wait 2 cycles"]
                    S2["SUB reads R1<br/>after it's written"]
                    S1 --> S2
                end
                
                subgraph Solution2["SOLUTION 2: FORWARDING"]
                    F1["Bypass E stage result<br/>directly to R stage"]
                    F2["No stall needed!"]
                    F1 --> F2
                end
                
                Problem --> Solution1
                Problem --> Solution2
                
                style Problem fill:#ffcdd2
                style Solution1 fill:#fff3e0
                style Solution2 fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 6.4: Data hazard and resolution strategies</div>

        <h3>Data Hazard Detection</h3>

        <div class="code-block">
<pre>module hazard_detector (
    input [3:0] src1, src2,           // Source registers from decode
    input [15:0] pending_writes,      // Bit i = is register i pending?
    output reg hazard                 // Is there a hazard?
);
    always @(*) begin
        hazard = (pending_writes[src1] || pending_writes[src2]);
        // If either source register is pending, there's a hazard
    end
endmodule</pre>
        </div>

        <h3>Forwarding (Operand Bypass)</h3>

        <div class="code-block">
<pre>module forwarding_logic (
    input [31:0] execute_result,      // Result from E stage
    input [31:0] memory_result,       // Result from M stage
    input [3:0] src_reg,              // Source register we need
    input [3:0] execute_dst,          // Dest of E stage instruction
    input [3:0] memory_dst,           // Dest of M stage instruction
    output reg [31:0] operand         // Forwarded value
);
    always @(*) begin
        if (execute_dst == src_reg && execute_dst != 0) begin
            operand = execute_result;  // Forward from E stage
        end else if (memory_dst == src_reg && memory_dst != 0) begin
            operand = memory_result;   // Forward from M stage
        end else begin
            operand = register_file[src_reg];  // Use register file
        end
    end
endmodule</pre>
        </div>

        <h3>Control Hazards (Branches)</h3>

        <p>When the pipeline fetches instructions before knowing if a branch is taken:</p>

        <div class="mermaid">
            flowchart TB
                subgraph NoPred["WITHOUT BRANCH PREDICTION"]
                    BR["BRnzp 1,0,1, +10"]
                    ADD["ADD R1,R2,R3<br/>(fetched speculatively)"]
                    INSTR3["Instr 3<br/>(fetched)"]
                    BR --> ADD --> INSTR3
                    FLUSH["If branch taken:<br/>FLUSH pipeline!<br/>3-4 cycles wasted"]
                    ADD -.->|"Wasted"| FLUSH
                    INSTR3 -.->|"Wasted"| FLUSH
                end
                
                subgraph WithPred["WITH BRANCH PREDICTION"]
                    PRED["Predict: Not Taken"]
                    CORRECT["If correct:<br/>No penalty! ✓"]
                    WRONG["If wrong:<br/>Flush & restart"]
                    PRED --> CORRECT
                    PRED --> WRONG
                end
                
                style NoPred fill:#ffcdd2
                style WithPred fill:#e8f5e9
                style FLUSH fill:#ef9a9a
                style CORRECT fill:#a5d6a7
        </div>
        <div class="figure-caption">Figure 6.5: Control hazard (branch misprediction penalty)</div>

        <h3>Structural Hazards</h3>

        <p>When multiple instructions need the same hardware resource:</p>

        <ul>
            <li><strong>Register file port conflict:</strong> Two warps try to write at the same time</li>
            <li><strong>Memory system contention:</strong> Multiple warps accessing L1 cache simultaneously</li>
            <li><strong>ALU resource conflict:</strong> More instructions than available ALUs</li>
        </ul>

        <h2>6.4 Branch Handling and Reconvergence</h2>

        <h3>The Challenge: Thread Divergence</h3>

        <div class="code-block">
<pre>// Kernel code with branch
if (threadIdx.x < 16) {
    expensiveComputation();
} else {
    simpleComputation();
}
// Reconvergence point: both paths meet here</pre>
        </div>

        <h3>The Reconvergence Stack</h3>

        <div class="mermaid">
            flowchart TB
                subgraph Initial["INITIAL STATE"]
                    S0["Stack Entry 0<br/>PC: 0x100<br/>Mask: 0xFFFFFFFF<br/>(all 32 active)"]
                end
                
                subgraph Branch["AT BRANCH (BRnzp)"]
                    EVAL["Evaluate condition<br/>for all 32 threads"]
                    T0_15["Threads 0-15:<br/>condition TRUE"]
                    T16_31["Threads 16-31:<br/>condition FALSE"]
                    EVAL --> T0_15
                    EVAL --> T16_31
                end
                
                subgraph Diverged["DIVERGED STACK"]
                    S1["Stack Entry 1 (top)<br/>Reconverge PC: 0x110<br/>Taken: 0x0000FFFF<br/>Not-taken: 0xFFFF0000"]
                    S0B["Stack Entry 0<br/>PC: 0x100<br/>Mask: 0xFFFFFFFF"]
                    S1 --> S0B
                end
                
                subgraph Execution["SERIAL EXECUTION"]
                    E1["Execute TAKEN path<br/>Mask: 0x0000FFFF<br/>(threads 0-15 active)"]
                    E2["Execute NOT-TAKEN<br/>Mask: 0xFFFF0000<br/>(threads 16-31 active)"]
                    RECONV["Pop stack<br/>Mask: 0xFFFFFFFF<br/>All reconverged!"]
                    E1 --> E2 --> RECONV
                end
                
                Initial --> Branch --> Diverged --> Execution
                
                style Initial fill:#e3f2fd
                style Diverged fill:#fff3e0
                style RECONV fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 6.6: Reconvergence stack operation for thread divergence</div>

        <h3>Reconvergence Stack Implementation</h3>

        <div class="code-block">
<pre>typedef struct {
    logic [31:0] pc;              // Reconvergence address
    logic [31:0] taken_mask;      // Active threads for taken path
    logic [31:0] not_taken_mask;  // Active threads for not-taken
    logic [31:0] not_taken_pc;    // Address of not-taken branch
} ReconvergenceEntry;

module reconvergence_stack #(
    parameter DEPTH = 16  // Support up to 16 nested branches
) (
    input clk, reset,
    input branch_detect,
    input [31:0] condition_mask,
    input [31:0] branch_target,
    input [31:0] not_taken_addr,
    input [31:0] reconverge_addr,
    
    output logic [31:0] active_mask,
    output logic [31:0] next_pc
);
    ReconvergenceEntry stack [DEPTH-1:0];
    logic [$clog2(DEPTH)-1:0] sp;  // Stack pointer
    
    always @(posedge clk or posedge reset) begin
        if (reset) begin
            sp <= 0;
            active_mask <= 32'hFFFFFFFF;
        end else if (branch_detect) begin
            // Push divergence point
            stack[sp].pc <= reconverge_addr;
            stack[sp].taken_mask <= condition_mask;
            stack[sp].not_taken_mask <= ~condition_mask;
            stack[sp].not_taken_pc <= not_taken_addr;
            sp <= sp + 1;
            active_mask <= condition_mask;  // Execute taken path
        end
    end
endmodule</pre>
        </div>

        <h2>6.5 Instruction Fetch Complications</h2>

        <h3>I-Cache Misses</h3>

        <div class="mermaid">
            flowchart LR
                subgraph Miss["I-CACHE MISS SCENARIO"]
                    PC["PC = 0x500<br/>(not in I-cache)"]
                    L2["Request L2 cache<br/>(~50 cycles)"]
                    FILL["Fill I-cache"]
                    RETRY["Retry fetch"]
                    PC --> L2 --> FILL --> RETRY
                end
                
                subgraph Hiding["LATENCY HIDING"]
                    STALL["Fetch stalls<br/>for ~50 cycles"]
                    OTHER["Other warps<br/>continue executing!"]
                    STALL -.-> OTHER
                end
                
                style Miss fill:#ffcdd2
                style Hiding fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 6.7: I-cache miss handling with latency hiding</div>

        <h2>6.6 Synchronization Instructions (SYNC)</h2>

        <div class="code-block">
<pre>module sync_barrier (
    input clk, reset,
    input [31:0] thread_mask,     // Which threads are active
    input sync_instr,             // SYNC instruction
    output reg all_threads_here   // Are all threads at barrier?
);
    logic [31:0] sync_mask;  // Threads that reached SYNC
    
    always @(posedge clk or posedge reset) begin
        if (reset) begin
            sync_mask <= 32'h0;
        end else if (sync_instr) begin
            sync_mask <= sync_mask | thread_mask;
        end
    end
    
    assign all_threads_here = (sync_mask == thread_mask);
    
    always @(posedge clk) begin
        if (all_threads_here) sync_mask <= 32'h0;
    end
endmodule</pre>
        </div>

        <h2>6.7 Pipeline Performance Analysis</h2>

        <h3>Key Metrics</h3>

        <div class="mermaid">
            flowchart LR
                subgraph CPI["CPI (Cycles Per Instruction)"]
                    IDEAL["Ideal CPI = 1.0<br/>(one instr/cycle)"]
                    REAL["Real CPI = 1.0 + stalls<br/>e.g., 1.2 with 20% stalls"]
                end
                
                subgraph Throughput["THROUGHPUT"]
                    TP["Throughput = 1/CPI<br/>= Instructions/Cycle"]
                    EX["1000 instrs, CPI=1.5<br/>→ 667 instrs/cycle"]
                    TP --> EX
                end
                
                style IDEAL fill:#c8e6c9
                style REAL fill:#fff3e0
        </div>

        <h2>6.8 Real GPU Pipeline Examples</h2>

        <h3>NVIDIA RTX 4090 Pipeline</h3>

        <ul>
            <li><strong>Stages:</strong> 8-12 stages (proprietary design)</li>
            <li><strong>Width:</strong> 128 FP32 operations per clock</li>
            <li><strong>Features:</strong> Aggressive out-of-order execution, massive register file (256KB per SM)</li>
            <li><strong>Cache:</strong> 128KB L1 per SM, 12MB L2 shared</li>
        </ul>

        <h3>AMD MI250X Pipeline</h3>

        <ul>
            <li><strong>Stages:</strong> Simpler pipeline, wavefront-friendly</li>
            <li><strong>Width:</strong> 64-wide wavefronts (vs NVIDIA's 32-wide warps)</li>
            <li><strong>Vector ALUs:</strong> 64 FP32 ALUs per CU</li>
            <li><strong>Cache:</strong> 64KB LDS per CU, 16MB L2</li>
        </ul>

        <h2>6.9 Chapter Summary</h2>

        <div class="key-takeaway">
            <strong>Pipeline stages:</strong> Fetch → Decode → Issue → Read → Execute → Memory → Writeback. 
            Each stage operates in parallel for different instructions.
        </div>

        <div class="key-takeaway">
            <strong>Hazards:</strong> Data (RAW), Control (branches), Structural (resource conflicts). 
            Each requires different solutions: forwarding, prediction, scheduling.
        </div>

        <div class="key-takeaway">
            <strong>Branch handling:</strong> Reconvergence stack tracks divergence/convergence points.
        </div>

        <div class="key-takeaway">
            <strong>Performance:</strong> CPI and throughput measure pipeline efficiency. 
            Goal is CPI ≈ 1.0 through hazard elimination and latency hiding.
        </div>

        <div class="exercise">
            <strong>Exercise 6.1:</strong> Analyze this code for hazards:
            <pre style="background: #f5f5f5; padding: 10px; margin-top: 10px;">
ADD R1, R2, R3
AND R4, R1, R5
OR  R1, R6, R7
XOR R8, R1, R9</pre>
        </div>

        <div class="exercise">
            <strong>Exercise 6.2:</strong> Design a branch predictor that uses a simple 2-bit counter per branch. 
            How would you integrate it into the pipeline?
        </div>

        <div class="exercise">
            <strong>Exercise 6.3:</strong> Calculate CPI for a kernel with:
            <ul>
                <li>1000 instructions</li>
                <li>100 LOAD instructions (take 200 cycles each)</li>
                <li>Assume perfect latency hiding among 8 warps</li>
            </ul>
        </div>

        <h2>Further Reading</h2>
        <ul>
            <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" target="_blank">NVIDIA CUDA Programming Guide</a></li>
            <li>"Computer Architecture: A Quantitative Approach" — Pipeline design chapter</li>
            <li><a href="https://github.com/adam-maj/tiny-gpu/blob/master/src/scheduler.sv" target="_blank">tiny-gpu scheduler.sv</a> — Real implementation</li>
        </ul>

        <div class="chapter-nav">
            <a href="chapter-05.html">← Previous: ISA Design</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-07.html">Next: Memory Hierarchy →</a>
        </div>
    </div>

    <script src="../script.js"></script>
    <script src="../navigation.js"></script>
</body>
</html>
