<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 7: Memory Hierarchy | Create Your Own GPU</title>
    <link rel="stylesheet" href="../styles.css" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
    <style>
        .chapter-content { max-width: 900px; margin: 0 auto; padding: 40px 20px; }
        .chapter-nav { display: flex; justify-content: space-between; margin: 40px 0; padding: 20px; background: var(--panel); border-radius: var(--radius); }
        .figure { margin: 30px 0; padding: 20px; background: #f9f9ff; border-radius: 12px; overflow-x: auto; }
        .figure-caption { font-style: italic; color: var(--muted); margin: 10px 0 0 0; text-align: center; }
        .key-takeaway { background: #e8f4fd; border-left: 4px solid #2196f3; padding: 16px; margin: 24px 0; }
        .exercise { background: #fff3e0; border-left: 4px solid #ff9800; padding: 16px; margin: 24px 0; }
        .engineer-note { background: #f3e5f5; border-left: 4px solid #9c27b0; padding: 16px; margin: 24px 0; }
        .code-block { background: #1e1e1e; color: #d4d4d4; padding: 20px; border-radius: 8px; overflow-x: auto; margin: 20px 0; font-family: monospace; font-size: 13px; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background: #f5f5f5; font-weight: bold; }
        .mermaid { background: white; padding: 20px; border-radius: 8px; border: 1px solid #ddd; margin: 20px 0; }
        pre { font-size: 12px; line-height: 1.4; }
    </style>
</head>
<body>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'base', themeVariables: { primaryColor: '#e3f2fd', primaryTextColor: '#1565c0', primaryBorderColor: '#90caf9', lineColor: '#64b5f6', secondaryColor: '#f3e5f5', tertiaryColor: '#fff3e0' } });
    </script>

    <div class="chapter-content">
        <div class="chapter-nav">
            <a href="chapter-06.html">‚Üê Previous: Pipeline Design</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-08.html">Next: Memory Coalescing ‚Üí</a>
        </div>

        <h1>Chapter 7: Memory Hierarchy</h1>
        
        <div class="meta" style="color: var(--muted); margin: 20px 0;">
            <span>Part II: Execution & Scheduling</span> ‚Ä¢ 
            <span>Reading time: ~65 min</span>
        </div>

        <h2>Introduction</h2>
        <p>
            If pipelines are the "beating heart" of a GPU, the memory system is the "cardiovascular system" 
            that keeps blood (data) flowing. Without a well-designed memory hierarchy, even a fast pipeline 
            sits idle waiting for data.
        </p>

        <p>
            Memory access latency is the #1 killer of GPU performance. A LOAD instruction can take:
        </p>
        <ul>
            <li>1-2 cycles if data is in register file</li>
            <li>20-40 cycles if in L1 cache</li>
            <li>100-200 cycles if in L2 cache</li>
            <li>200-400 cycles if in main DRAM</li>
        </ul>

        <h2>7.1 The Memory Pyramid</h2>

        <div class="mermaid">
            flowchart TB
                subgraph Pyramid["MEMORY HIERARCHY PYRAMID"]
                    REG["üî¥ Registers (256 KB/SM)<br/>1 cycle latency<br/>32 KB/cycle bandwidth<br/>Per-thread"]
                    L1["üü† L1 Cache (128 KB/SM)<br/>20-40 cycles<br/>128 B/cycle<br/>Per-SM"]
                    L2["üü° L2 Cache (12-40 MB)<br/>100-200 cycles<br/>256 B/cycle<br/>All SMs shared"]
                    DRAM["üü¢ HBM/DRAM (8-80 GB)<br/>200-400 cycles<br/>250-900 GB/s<br/>All SMs shared"]
                    
                    REG --> L1 --> L2 --> DRAM
                end
                
                style REG fill:#ffcdd2
                style L1 fill:#ffe0b2
                style L2 fill:#fff9c4
                style DRAM fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 7.1: Memory hierarchy pyramid ‚Äî Bigger = Slower (exponential tradeoff)</div>

        <h3>Capacity and Bandwidth Overview</h3>

        <table>
            <thead>
                <tr>
                    <th>Level</th>
                    <th>Capacity</th>
                    <th>Latency</th>
                    <th>Bandwidth</th>
                    <th>Scope</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>Registers</td><td>256 KB (per SM)</td><td>0-1 cy</td><td>32 KB/cy</td><td>Per-thread</td></tr>
                <tr><td>L1 Cache</td><td>128 KB (per SM)</td><td>20-40 cy</td><td>128 B/cy</td><td>Per-SM</td></tr>
                <tr><td>L2 Cache</td><td>12-40 MB</td><td>100-200 cy</td><td>256 B/cy</td><td>All SMs</td></tr>
                <tr><td>DRAM</td><td>8-80 GB</td><td>200-400 cy</td><td>250-900 GB/s</td><td>All SMs</td></tr>
            </tbody>
        </table>

        <h2>7.2 Cache Fundamentals</h2>

        <h3>Cache Parameters and Design Space</h3>

        <ul>
            <li><strong>Size:</strong> Total capacity (KB or MB). Larger = fewer misses, but slower access and more power.</li>
            <li><strong>Line size:</strong> Minimum unit of data movement (32-128 bytes typical).</li>
            <li><strong>Associativity:</strong> How many places a given address can map to.</li>
            <li><strong>Replacement policy:</strong> Which line to evict on a miss (LRU, FIFO).</li>
        </ul>

        <h3>L1 Cache Internals</h3>

        <div class="code-block">
<pre>L1 Cache Parameters:
  Size = 128 KB = 131,072 bytes
  Line size = 128 bytes
  Number of lines = 131,072 / 128 = 1024 lines
  Associativity = 4-way
  Number of sets = 1024 / 4 = 256 sets

Address to Cache Index:
  Physical Address (32-bit)
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ [31:11] Tag | [10:7] Index | [6:0] Offset ‚îÇ
  ‚îÇ  21 bits    ‚îÇ  4 bits     ‚îÇ   7 bits     ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>
        </div>

        <h3>Cache Lookup Process</h3>

        <div class="mermaid">
            flowchart TB
                subgraph Lookup["CACHE LOOKUP EXAMPLE"]
                    ADDR["Address: 0x12345678"]
                    EXTRACT["Extract fields:<br/>Tag=0x1234, Index=5, Offset=0x78"]
                    SET["Look up Set 5<br/>(4-way associative)"]
                    
                    ADDR --> EXTRACT --> SET
                end
                
                subgraph Ways["SET 5 CONTENTS"]
                    W0["Way 0: Tag=0xABCD ‚ùå"]
                    W1["Way 1: Tag=0x1234 ‚úì MATCH!"]
                    W2["Way 2: Tag=0x5678 ‚ùå"]
                    W3["Way 3: Invalid"]
                end
                
                subgraph Result["RESULT"]
                    HIT["CACHE HIT!<br/>Return data[0x78]<br/>Latency: ~20 cycles"]
                    MISS["If no match:<br/>CACHE MISS<br/>Fetch from L2"]
                end
                
                SET --> Ways
                W1 --> HIT
                W0 & W2 & W3 -.-> MISS
                
                style HIT fill:#c8e6c9
                style MISS fill:#ffcdd2
                style W1 fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 7.2: Cache lookup walkthrough</div>

        <h2>7.3 Cache Performance Metrics</h2>

        <h3>Hit Rate and Average Access Time</h3>

        <div class="code-block">
<pre>Definitions:
  Hit = Requested data found in cache
  Miss = Not found, must fetch from next level
  
  Hit rate = (Total hits) / (Total accesses)
  Miss rate = 1 - Hit rate
  
Average Access Time (AAT):
  AAT = Hit_rate √ó L1_latency + Miss_rate √ó L2_latency
      = 0.90 √ó 20 + 0.10 √ó 150
      = 18 + 15
      = 33 cycles
      
Without cache:
  AAT = 150 cycles (always fetch from L2)
  
Speedup = 150 / 33 ‚âà 4.5√ó

A 90% hit rate with cache gives 4.5√ó speedup over no cache!</pre>
        </div>

        <h2>7.4 Shared Memory and Programmer Control</h2>

        <h3>What is Shared Memory?</h3>

        <div class="mermaid">
            flowchart LR
                subgraph Auto["AUTOMATIC: L1 CACHE"]
                    A1["Hardware-managed"]
                    A2["Transparent to programmer"]
                    A3["20-40 cycles latency"]
                end
                
                subgraph Manual["PROGRAMMER-CONTROLLED: SHARED MEMORY"]
                    M1["48-96 KB per SM"]
                    M2["Explicit allocation"]
                    M3["1-2 cycles latency!"]
                    M4["Shared by block threads"]
                end
                
                style Manual fill:#c8e6c9
                style Auto fill:#fff3e0
        </div>

        <h3>Shared Memory Usage Pattern</h3>

        <div class="code-block">
<pre>// CUDA: Matrix multiplication with shared memory tiling
__global__ void matmul_tiled(float *A, float *B, float *C, int N) {
    __shared__ float tileA[32][32];  // 4 KB
    __shared__ float tileB[32][32];  // 4 KB
    
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    float sum = 0.0f;
    
    for (int t = 0; t < N/32; t++) {
        // PHASE 1: Load global ‚Üí shared (parallel)
        tileA[tid/32][tid%32] = A[...];
        tileB[tid/32][tid%32] = B[...];
        
        __syncthreads();  // Barrier: wait for all threads
        
        // PHASE 2: Compute using shared (many accesses)
        for (int k = 0; k < 32; k++)
            sum += tileA[tid/32][k] * tileB[k][tid%32];
        
        __syncthreads();
    }
    
    C[...] = sum;
}

Performance impact:
  Phase 1: 1 global load per element ~150 cy
  Phase 2: 1024 shared accesses per element ~2 cy each
  
  Without shared memory: 32√ó32√ó2 global reads ‚âà 204,800 cycles
  With shared memory: 2474 cycles
  Speedup: 82.8√ó!</pre>
        </div>

        <h2>7.5 L2 Cache and Design</h2>

        <ul>
            <li>12-40 MB (shared across all SMs)</li>
            <li>16-way or higher associativity</li>
            <li>Larger line size (256-512 bytes)</li>
            <li>100-200 cycle latency</li>
        </ul>

        <div class="engineer-note">
            <strong>Key Insight:</strong> If your L2 hit rate is >80%, most DRAM bandwidth 
            is wasted. If L2 hit rate is <30%, you're DRAM-bound ‚Äî optimize access patterns.
        </div>

        <h2>7.6 Main Memory: DRAM and HBM</h2>

        <h3>High Bandwidth Memory (HBM)</h3>

        <div class="mermaid">
            flowchart LR
                subgraph DDR["STANDARD DDR (CPU)"]
                    D1["50-100 GB/s bandwidth"]
                    D2["1-2 channels"]
                    D3["64-128 bit bus"]
                end
                
                subgraph HBM2["HBM2 (GPU)"]
                    H1["250-410 GB/s bandwidth"]
                    H2["8 channels"]
                    H3["1024 bit bus!"]
                end
                
                subgraph HBM3["HBM3 (Modern GPU)"]
                    H31["600-900 GB/s bandwidth"]
                    H32["16 channels"]
                    H33["3D stacked memory"]
                end
                
                style DDR fill:#ffcdd2
                style HBM2 fill:#fff9c4
                style HBM3 fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 7.3: HBM vs DDR memory comparison</div>

        <h3>Memory Controller Architecture</h3>

        <div class="mermaid">
            flowchart TB
                L2["From L2 Cache<br/>(256-512 requests/cycle)"]
                RQ["Request Queue<br/>(~256 entries)"]
                TLB["Address Translation<br/>(TLB lookup)"]
                SCHED["Bank/Channel Scheduler<br/>Distribute to 8 channels"]
                
                L2 --> RQ --> TLB --> SCHED
                
                SCHED --> CH0["CH0"]
                SCHED --> CH1["CH1"]
                SCHED --> CH2["CH2"]
                SCHED --> CH3["..."]
                SCHED --> CH7["CH7"]
                
                BW["Aggregate: 900 GB/s<br/>(all 8 channels parallel)"]
                
                CH0 & CH1 & CH2 & CH3 & CH7 --> BW
                
                style L2 fill:#e3f2fd
                style BW fill:#c8e6c9
        </div>
        <div class="figure-caption">Figure 7.4: HBM8 memory controller block diagram</div>

        <h2>7.7 Bandwidth Utilization and Bottlenecks</h2>

        <h3>Compute-Bound vs Memory-Bound Kernels</h3>

        <div class="mermaid">
            flowchart LR
                subgraph Analysis["OPERATIONAL INTENSITY ANALYSIS"]
                    OI["Operational Intensity<br/>= FLOPs / Bytes"]
                end
                
                subgraph MemBound["MEMORY-BOUND<br/>(Intensity < 1)"]
                    MB1["Vector scale: y = a*x"]
                    MB2["1 FLOP / 8 bytes = 0.125"]
                    MB3["Limited by bandwidth"]
                end
                
                subgraph CompBound["COMPUTE-BOUND<br/>(Intensity > 5)"]
                    CB1["Matrix multiply: C = A√óB"]
                    CB2["65K FLOPs / 8KB = 8"]
                    CB3["Limited by ALUs"]
                end
                
                Analysis --> MemBound
                Analysis --> CompBound
                
                style MemBound fill:#ffcdd2
                style CompBound fill:#c8e6c9
        </div>

        <h2>7.8 Bank Conflicts and Shared Memory Optimization</h2>

        <h3>Bank Conflict Problem</h3>

        <div class="mermaid">
            flowchart TB
                subgraph Banks["SHARED MEMORY: 32 BANKS"]
                    B0["Bank 0:<br/>addr % 32 == 0"]
                    B1["Bank 1:<br/>addr % 32 == 4"]
                    B2["Bank 2:<br/>addr % 32 == 8"]
                    B31["Bank 31:<br/>addr % 32 == 124"]
                end
                
                subgraph Good["GOOD ACCESS PATTERN"]
                    G1["Thread 0 ‚Üí Bank 0 ‚úì"]
                    G2["Thread 1 ‚Üí Bank 1 ‚úì"]
                    G3["Thread 2 ‚Üí Bank 2 ‚úì"]
                    G4["All 32 threads ‚Üí different banks"]
                    G5["No conflicts! 32√ó parallel"]
                end
                
                subgraph Bad["BAD ACCESS PATTERN"]
                    B1x["Thread 0 ‚Üí Bank 0 ‚úì"]
                    B2x["Thread 1 ‚Üí Bank 0 ‚úó CONFLICT!"]
                    B3x["Thread 2 ‚Üí Bank 0 ‚úó CONFLICT!"]
                    B4x["Serialized: 32 cycles!"]
                end
                
                style Good fill:#c8e6c9
                style Bad fill:#ffcdd2
        </div>
        <div class="figure-caption">Figure 7.5: Shared memory bank conflicts</div>

        <h3>Avoiding Bank Conflicts</h3>

        <div class="code-block">
<pre>// BAD: All threads access same location
__shared__ float data[32];
float x = data[0];  // 32 threads ‚Üí same bank conflict!

// GOOD: Staggered access across banks
__shared__ float data[32];
int tid = threadIdx.x;
float x = data[tid];  // Each thread gets different bank

// GOOD: 2D array with column access
__shared__ float matrix[32][32];
int row = threadIdx.y;
int col = threadIdx.x;
float x = matrix[row][col];  // Stride across banks</pre>
        </div>

        <h2>7.9 Case Study: Tiny GPU Memory System</h2>

        <p>Tiny GPU uses a simplified memory model (for learning):</p>

        <ul>
            <li>Only two memory levels: registers + global DRAM</li>
            <li>No L1/L2 caches (to reduce RTL complexity)</li>
            <li>Simple memory controller with single read/write channel</li>
            <li>Long latency (200+ cycles) to model real DRAM</li>
        </ul>

        <div class="code-block">
<pre>// Tiny GPU memory flow (from src/controller.sv)
module controller #(
    parameter DATA_WIDTH = 32,
    parameter ADDR_WIDTH = 32
) (
    input clk, reset,
    
    // From cores (multiple read/write requests)
    input [NUM_CORES-1:0] mem_write_valid,
    input [NUM_CORES-1:0][ADDR_WIDTH-1:0] mem_write_addr,
    input [NUM_CORES-1:0][DATA_WIDTH-1:0] mem_write_data,
    
    input [NUM_CORES-1:0] mem_read_valid,
    input [NUM_CORES-1:0][ADDR_WIDTH-1:0] mem_read_addr,
    output [NUM_CORES-1:0] mem_read_ready,
    output [NUM_CORES-1:0][DATA_WIDTH-1:0] mem_read_data,
    
    // To external DRAM simulation
    output reg dram_write_valid,
    output reg [ADDR_WIDTH-1:0] dram_addr,
    output reg [DATA_WIDTH-1:0] dram_data
);
// Simple round-robin arbiter for multiple requests
endmodule</pre>
        </div>

        <h2>Key Takeaways</h2>

        <div class="key-takeaway">
            <strong>1. Memory hierarchy is critical:</strong> A single DRAM miss (~400 cycles) stalls 
            your entire GPU. L1/L2 caches hide this latency by catching ~95% of accesses early.
        </div>

        <div class="key-takeaway">
            <strong>2. Bandwidth ‚â† Latency:</strong> HBM gives 900 GB/s but still takes 
            ~200 cycles to fetch data. Latency matters for small working sets; bandwidth for streaming.
        </div>

        <div class="key-takeaway">
            <strong>3. Shared memory is programmer-controlled cache:</strong> Use it for 
            data reuse and to reduce global memory traffic (the #1 bottleneck).
        </div>

        <div class="key-takeaway">
            <strong>4. Operational intensity determines performance:</strong> A kernel with 
            >5 FLOPs/byte is compute-bound; <1 FLOP/byte is memory-bound. Optimize accordingly.
        </div>

        <h2>Exercises</h2>

        <div class="exercise">
            <strong>Exercise 7.1:</strong> Calculate the average access time (AAT) for a system with:
            <ul>
                <li>L1 hit rate: 92%</li>
                <li>L1 latency: 20 cycles</li>
                <li>L2 latency: 150 cycles</li>
            </ul>
            What speedup does this cache hierarchy provide vs. no cache?
        </div>

        <div class="exercise">
            <strong>Exercise 7.2:</strong> A kernel transfers 2 GB of data and performs 100 billion FLOPs. 
            Is it compute-bound or memory-bound on an H100 (960 GB/s, 141 TFLOPS)?
        </div>

        <div class="exercise">
            <strong>Exercise 7.3:</strong> Write CUDA code to multiply two 256√ó256 matrices using 
            shared memory tiling (32√ó32 blocks). Estimate the speedup over naive global memory access.
        </div>

        <h2>Further Reading</h2>

        <ul>
            <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank">NVIDIA CUDA Toolkit Documentation</a></li>
            <li>GPU Gems 3, Chapter 29: "Optimizing CUDA"</li>
            <li>The CUDA Handbook by N. Wilt</li>
            <li><a href="https://github.com/gpucci/gpgpu-sim_distribution" target="_blank">GPGPU-Sim</a> ‚Äî model memory hierarchy behavior</li>
        </ul>

        <div class="chapter-nav">
            <a href="chapter-06.html">‚Üê Previous: Pipeline Design</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-08.html">Next: Memory Coalescing ‚Üí</a>
        </div>
    </div>

    <script src="../script.js"></script>
    <script src="../navigation.js"></script>
</body>
</html>
