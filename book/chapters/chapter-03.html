<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 3: Parallel Execution Models | Create Your Own GPU</title>
    <link rel="stylesheet" href="../styles.css" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
    <style>
        .chapter-content { max-width: 800px; margin: 0 auto; padding: 40px 20px; }
        .chapter-nav { display: flex; justify-content: space-between; margin: 40px 0; padding: 20px; background: var(--panel); border-radius: var(--radius); }
        .figure { margin: 30px 0; padding: 20px; background: #f9f9ff; border-radius: 12px; }
        .figure-caption { font-style: italic; color: var(--muted); margin-top: 10px; text-align: center; }
        .key-takeaway { background: #e8f4fd; border-left: 4px solid #2196f3; padding: 16px; margin: 24px 0; }
        .exercise { background: #fff3e0; border-left: 4px solid #ff9800; padding: 16px; margin: 24px 0; }
        .engineer-note { background: #f3e5f5; border-left: 4px solid #9c27b0; padding: 16px; margin: 24px 0; }
        .mermaid { background: transparent; text-align: center; }
        .code-block { background: #1e1e1e; color: #d4d4d4; padding: 20px; border-radius: 8px; overflow-x: auto; margin: 20px 0; font-family: 'Consolas', 'Monaco', monospace; font-size: 14px; }
        .comparison-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        .comparison-table th, .comparison-table td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        .comparison-table th { background: #f1f5f9; }
        .comparison-table tr:nth-child(even) { background: #f9fafb; }
        .concept-card { padding: 20px; margin: 15px 0; border-radius: 12px; border-left: 4px solid; }
        .concept-card.simd { background: #eef2ff; border-color: #6366f1; }
        .concept-card.simt { background: #ecfdf5; border-color: #10b981; }
    </style>
</head>
<body>
    <div class="chapter-content">
        <div class="chapter-nav">
            <a href="chapter-02.html">‚Üê Previous: GPU vs CPU</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-04.html">Next: Core Components ‚Üí</a>
        </div>

        <h1>Chapter 3: Parallel Execution Models</h1>
        
        <div class="meta" style="color: var(--muted); margin: 20px 0;">
            <span>Part I: GPU Fundamentals</span> ‚Ä¢ 
            <span>Reading time: ~60 min</span>
        </div>

        <h2>Introduction</h2>
        <p>
            We've established that GPUs are parallel processors, but <em>how</em> do they achieve parallelism? 
            This chapter dives deep into the execution models that make GPUs tick: SIMD, SIMT, warps, thread 
            blocks, and the programming abstractions built on top of them.
        </p>

        <p>
            Understanding these concepts is critical whether you're programming GPUs or designing them. 
            The execution model determines everything from hardware design choices to optimal coding patterns.
        </p>

        <h2>3.1 SIMD: The Foundation</h2>

        <p>
            <strong>SIMD (Single Instruction, Multiple Data)</strong> is the oldest and simplest form of 
            data parallelism. One instruction operates on multiple data elements simultaneously.
        </p>

        <div class="concept-card simd">
            <h4>üî¢ SIMD Concept</h4>
            <p>Instead of:</p>
            <code>for (i=0; i&lt;4; i++) C[i] = A[i] + B[i];</code>
            <p style="margin-top: 10px;">Execute a single vector instruction:</p>
            <code>VADD C[0:3], A[0:3], B[0:3]</code>
        </div>

        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph SCALAR["Scalar Execution (4 cycles)"]
        direction TB
        S1["ADD C[0], A[0], B[0]"] --> S2["ADD C[1], A[1], B[1]"]
        S2 --> S3["ADD C[2], A[2], B[2]"]
        S3 --> S4["ADD C[3], A[3], B[3]"]
    end
    
    subgraph SIMD["SIMD Execution (1 cycle)"]
        direction TB
        V1["VADD"]
        A0["A[0]"] --> V1
        A1["A[1]"] --> V1
        A2["A[2]"] --> V1
        A3["A[3]"] --> V1
        B0["B[0]"] --> V1
        B1["B[1]"] --> V1
        B2["B[2]"] --> V1
        B3["B[3]"] --> V1
        V1 --> C0["C[0]"]
        V1 --> C1["C[1]"]
        V1 --> C2["C[2]"]
        V1 --> C3["C[3]"]
    end
    
    style SCALAR fill:#fee2e2,stroke:#ef4444
    style SIMD fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.1: Scalar vs SIMD execution ‚Äî 4 operations in 1 cycle vs 4 cycles</div>
        </div>

        <p>
            SIMD exists in CPUs too (SSE, AVX, NEON), but GPUs take it much further with wider vectors 
            and more aggressive parallelism.
        </p>

        <h2>3.2 SIMT: NVIDIA's Innovation</h2>

        <p>
            <strong>SIMT (Single Instruction, Multiple Threads)</strong> is NVIDIA's programming model 
            innovation that makes GPU programming feel like writing threaded code while executing as SIMD.
        </p>

        <div class="concept-card simt">
            <h4>‚ö° SIMT Concept</h4>
            <p><strong>Software view:</strong> Write code for a single thread, launch thousands of independent threads.</p>
            <p><strong>Hardware view:</strong> Groups of threads (warps) execute the same instruction in lockstep SIMD.</p>
        </div>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph SW["Software View: Independent Threads"]
        direction LR
        T0["Thread 0<br/>C[0] = A[0] + B[0]"]
        T1["Thread 1<br/>C[1] = A[1] + B[1]"]
        T2["Thread 2<br/>C[2] = A[2] + B[2]"]
        TN["Thread N<br/>C[N] = A[N] + B[N]"]
    end
    
    subgraph HW["Hardware View: Warp Execution"]
        direction TB
        WARP["Warp (32 threads)"]
        INST["Single ADD instruction"]
        ALU0["ALU 0"] 
        ALU1["ALU 1"]
        ALU2["ALU ..."]
        ALU31["ALU 31"]
        
        WARP --> INST
        INST --> ALU0
        INST --> ALU1
        INST --> ALU2
        INST --> ALU31
    end
    
    SW --> HW
    
    style SW fill:#dbeafe,stroke:#3b82f6
    style HW fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.2: SIMT ‚Äî threads in software, SIMD in hardware</div>
        </div>

        <h3>Why SIMT is Powerful</h3>
        <ul>
            <li><strong>Easier programming:</strong> Write code for one thread, hardware parallelizes it</li>
            <li><strong>Flexible parallelism:</strong> Launch 1,000 or 1,000,000 threads ‚Äî same code!</li>
            <li><strong>Per-thread state:</strong> Each thread has its own registers and PC</li>
            <li><strong>Graceful divergence:</strong> Threads can take different paths (more on this later)</li>
        </ul>

        <div class="key-takeaway">
            <strong>Key Insight:</strong> SIMT is SIMD in hardware, threads in software. The programmer thinks 
            in threads; the hardware executes in lockstep groups (warps/wavefronts).
        </div>

        <h2>3.3 The GPU Thread Hierarchy</h2>

        <p>
            GPU programming models organize threads into a hierarchy. Let's use CUDA terminology (the most common), 
            but know that OpenCL and HIP use similar concepts with different names.
        </p>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph GRID["Grid (Entire Kernel Launch)"]
        subgraph BLOCK0["Block (0,0)"]
            subgraph WARP0["Warp 0"]
                T0["T0"] ~~~ T1["T1"] ~~~ T2["..."] ~~~ T31["T31"]
            end
            subgraph WARP1["Warp 1"]
                T32["T32"] ~~~ T33["T33"] ~~~ T34["..."] ~~~ T63["T63"]
            end
        end
        subgraph BLOCK1["Block (0,1)"]
            W2["Warps..."]
        end
        subgraph BLOCK2["Block (1,0)"]
            W3["Warps..."]
        end
        subgraph BLOCK3["Block (1,1)"]
            W4["Warps..."]
        end
    end
    
    style GRID fill:#f1f5f9,stroke:#64748b
    style BLOCK0 fill:#dbeafe,stroke:#3b82f6
    style BLOCK1 fill:#dbeafe,stroke:#3b82f6
    style BLOCK2 fill:#dbeafe,stroke:#3b82f6
    style BLOCK3 fill:#dbeafe,stroke:#3b82f6
    style WARP0 fill:#dcfce7,stroke:#22c55e
    style WARP1 fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.3: GPU thread hierarchy ‚Äî Grid ‚Üí Blocks ‚Üí Warps ‚Üí Threads</div>
        </div>

        <h3>Level 1: Thread</h3>
        <p>
            A <strong>thread</strong> is the smallest unit of execution. Each thread:
        </p>
        <ul>
            <li>Executes the kernel code (GPU function)</li>
            <li>Has its own registers (R0, R1, ... R255 in some GPUs)</li>
            <li>Has a unique thread ID</li>
            <li>Can read/write memory</li>
        </ul>

        <div class="code-block">
<pre>// CUDA kernel: each thread computes one element
__global__ void vectorAdd(float *a, float *b, float *c, int N) {
    int i = threadIdx.x;  // Thread's index within its block
    if (i < N) {
        c[i] = a[i] + b[i];
    }
}</pre>
        </div>

        <h3>Level 2: Warp (NVIDIA) / Wavefront (AMD)</h3>
        <p>
            A <strong>warp</strong> is a group of 32 threads (64 on AMD) that execute the same instruction together. 
            This is the hardware's SIMD width.
        </p>

        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph BLOCK["Thread Block (128 threads)"]
        subgraph W0["Warp 0: Threads 0-31"]
            direction TB
            I0["Instruction A"]
        end
        subgraph W1["Warp 1: Threads 32-63"]
            direction TB
            I1["Instruction B"]
        end
        subgraph W2["Warp 2: Threads 64-95"]
            direction TB
            I2["Instruction C"]
        end
        subgraph W3["Warp 3: Threads 96-127"]
            direction TB
            I3["Instruction A"]
        end
    end
    
    style BLOCK fill:#f1f5f9,stroke:#64748b
    style W0 fill:#dcfce7,stroke:#22c55e
    style W1 fill:#fef3c7,stroke:#f59e0b
    style W2 fill:#dbeafe,stroke:#3b82f6
    style W3 fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.4: Warps execute independently; threads within a warp execute in lockstep</div>
        </div>

        <div class="engineer-note">
            <strong>Hardware Detail:</strong> The warp size is determined by the width of your SIMD execution units. 
            If you have 32 ALUs, you can process 32 threads in parallel ‚Üí warp size = 32. This is a fundamental 
            hardware parameter you'll choose when designing your GPU.
        </div>

        <h3>Level 3: Thread Block / Work-Group</h3>
        <p>
            A <strong>thread block</strong> is a group of threads (typically 128, 256, or 512) that:
        </p>
        <ul>
            <li>Execute on the same GPU core (SM in NVIDIA terms)</li>
            <li>Share fast <strong>shared memory</strong></li>
            <li>Can synchronize with <code>__syncthreads()</code></li>
            <li>Are scheduled as a unit</li>
        </ul>

        <h3>Level 4: Grid</h3>
        <p>
            A <strong>grid</strong> is the entire set of thread blocks launched by a kernel.
        </p>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph GRID["Grid (4√ó3 = 12 Blocks)"]
        direction TB
        subgraph ROW0[" "]
            direction LR
            B00["Block<br/>(0,0)"] ~~~ B10["Block<br/>(1,0)"] ~~~ B20["Block<br/>(2,0)"] ~~~ B30["Block<br/>(3,0)"]
        end
        subgraph ROW1[" "]
            direction LR
            B01["Block<br/>(0,1)"] ~~~ B11["Block<br/>(1,1)"] ~~~ B21["Block<br/>(2,1)"] ~~~ B31["Block<br/>(3,1)"]
        end
        subgraph ROW2[" "]
            direction LR
            B02["Block<br/>(0,2)"] ~~~ B12["Block<br/>(1,2)"] ~~~ B22["Block<br/>(2,2)"] ~~~ B32["Block<br/>(3,2)"]
        end
    end
    
    NOTE["Total: 12 blocks √ó 256 threads = 3072 threads"]
    
    style GRID fill:#f1f5f9,stroke:#64748b
            </div>
            <div class="figure-caption">Figure 3.5: 2D Grid of thread blocks</div>
        </div>

        <h3>Thread Indexing</h3>
        <div class="code-block">
<pre>// 1D grid of 1D blocks
int globalThreadId = blockIdx.x * blockDim.x + threadIdx.x;

// 2D grid of 2D blocks (e.g., matrix operations)
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;

// 3D example (e.g., volumetric data)
int x = blockIdx.x * blockDim.x + threadIdx.x;
int y = blockIdx.y * blockDim.y + threadIdx.y;
int z = blockIdx.z * blockDim.z + threadIdx.z;</pre>
        </div>

        <table class="comparison-table">
            <thead>
                <tr>
                    <th>Level</th>
                    <th>CUDA Name</th>
                    <th>OpenCL Name</th>
                    <th>Typical Size</th>
                    <th>Hardware Mapping</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1</td>
                    <td>Thread</td>
                    <td>Work-Item</td>
                    <td>1</td>
                    <td>One lane in SIMD unit</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>Warp</td>
                    <td>Wavefront</td>
                    <td>32 (NVIDIA), 64 (AMD)</td>
                    <td>SIMD execution width</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>Block</td>
                    <td>Work-Group</td>
                    <td>128-1024 threads</td>
                    <td>One SM/CU (GPU core)</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>Grid</td>
                    <td>NDRange</td>
                    <td>1000s-1000000s</td>
                    <td>Entire GPU</td>
                </tr>
            </tbody>
        </table>

        <h2>3.4 Thread Divergence and Convergence</h2>

        <h3>The Divergence Problem</h3>
        <p>
            Since threads in a warp execute in lockstep (SIMT), what happens when they need to take different 
            paths? This is called <strong>thread divergence</strong>.
        </p>

        <div class="code-block">
<pre>__global__ void divergentKernel(int *data, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i % 2 == 0) {
        // Even threads: execute path A
        data[i] = computeA(data[i]);
    } else {
        // Odd threads: execute path B
        data[i] = data[i] * 2;
    }
}</pre>
        </div>

        <div class="figure">
            <div class="mermaid">
sequenceDiagram
    participant W as Warp (8 threads)
    participant IF as If-Branch
    participant ELSE as Else-Branch
    
    Note over W: Threads 0,2,4,6 = Even<br/>Threads 1,3,5,7 = Odd
    
    W->>IF: Step 1: Execute IF (Even threads active)
    Note right of IF: Threads 0,2,4,6 ACTIVE<br/>Threads 1,3,5,7 MASKED
    
    W->>ELSE: Step 2: Execute ELSE (Odd threads active)
    Note right of ELSE: Threads 0,2,4,6 MASKED<br/>Threads 1,3,5,7 ACTIVE
    
    Note over W: Step 3: Reconverge<br/>All threads active again
            </div>
            <div class="figure-caption">Figure 3.6: Thread divergence causes serialization ‚Äî both paths execute sequentially</div>
        </div>

        <h3>Hardware Mechanism: Active Masks</h3>
        <ul>
            <li>Each warp has a bitmask indicating which threads are active</li>
            <li>When branches diverge, the warp executes each path sequentially</li>
            <li>Inactive threads are masked off (results discarded)</li>
            <li>When paths reconverge, all threads are active again</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    START["All threads active<br/>Mask: 11111111"]
    
    BRANCH{"if (tid % 2 == 0)?"}
    
    IF_PATH["Execute IF branch<br/>Mask: 10101010<br/>(Even threads)"]
    ELSE_PATH["Execute ELSE branch<br/>Mask: 01010101<br/>(Odd threads)"]
    
    RECONVERGE["Reconverge<br/>Mask: 11111111"]
    
    START --> BRANCH
    BRANCH -->|Even| IF_PATH
    BRANCH -->|Odd| ELSE_PATH
    IF_PATH --> RECONVERGE
    ELSE_PATH --> RECONVERGE
    
    style IF_PATH fill:#dcfce7,stroke:#22c55e
    style ELSE_PATH fill:#fef3c7,stroke:#f59e0b
            </div>
            <div class="figure-caption">Figure 3.7: Active mask mechanism during divergence</div>
        </div>

        <div class="engineer-note">
            <strong>Design Decision:</strong> When building a GPU, you need to implement a <strong>reconvergence stack</strong> 
            in hardware. This keeps track of divergence points and ensures threads reconverge at the right place.
        </div>

        <h3>Divergence-Free Patterns</h3>

        <div class="code-block">
<pre>// BAD: Adjacent threads diverge
if (threadIdx.x % 2 == 0) { ... }

// GOOD: Entire warps stay together (warp size = 32)
if (threadIdx.x < 32) { ... }  // Warp 0: all true, Warp 1: all false

// BETTER: Predicated execution (no branch)
result = condition ? a : b;  // Compiles to select instruction</pre>
        </div>

        <h2>3.5 Programming Model Abstractions</h2>

        <h3>CUDA (NVIDIA)</h3>
        <div class="code-block">
<pre>// Complete CUDA example: vector addition
__global__ void vectorAdd(float *a, float *b, float *c, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    int N = 1<<20;  // 1 million elements
    
    // Allocate and copy data...
    
    // Launch kernel
    int threads = 256;
    int blocks = (N + threads - 1) / threads;
    vectorAdd<<<blocks, threads>>>(d_a, d_b, d_c, N);
    
    return 0;
}</pre>
        </div>

        <h3>OpenCL (Cross-Platform)</h3>
        <div class="code-block">
<pre>// OpenCL kernel (same logic)
__kernel void vectorAdd(__global float *a, __global float *b, 
                        __global float *c, int N) {
    int i = get_global_id(0);
    if (i < N) {
        c[i] = a[i] + b[i];
    }
}</pre>
        </div>

        <h3>Comparison Table</h3>
        <table class="comparison-table">
            <thead>
                <tr>
                    <th>Feature</th>
                    <th>CUDA</th>
                    <th>OpenCL</th>
                    <th>HIP</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Vendor</td>
                    <td>NVIDIA</td>
                    <td>Khronos (open)</td>
                    <td>AMD</td>
                </tr>
                <tr>
                    <td>Target GPUs</td>
                    <td>NVIDIA only</td>
                    <td>All vendors</td>
                    <td>AMD + NVIDIA</td>
                </tr>
                <tr>
                    <td>Ease of Use</td>
                    <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                    <td>‚≠ê‚≠ê‚≠ê</td>
                    <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                </tr>
                <tr>
                    <td>Ecosystem</td>
                    <td>Huge</td>
                    <td>Moderate</td>
                    <td>Growing</td>
                </tr>
            </tbody>
        </table>

        <h2>3.6 Kernel Launch Mechanics</h2>

        <p>
            Let's trace the journey from <code>kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(args)</code> to hardware execution:
        </p>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph HOST["Host (CPU)"]
        LAUNCH["cudaLaunchKernel()"]
    end
    
    subgraph DRIVER["CUDA Driver"]
        VALIDATE["Validate params"]
        PACKET["Build command packet"]
    end
    
    subgraph GPU["GPU Hardware"]
        CMD["Command Processor"]
        GMU["Grid Management Unit"]
        
        subgraph SMs["Streaming Multiprocessors"]
            SM0["SM 0<br/>Blocks 0,3,6..."]
            SM1["SM 1<br/>Blocks 1,4,7..."]
            SM2["SM 2<br/>Blocks 2,5,8..."]
        end
    end
    
    LAUNCH --> VALIDATE
    VALIDATE --> PACKET
    PACKET -->|PCIe/NVLink| CMD
    CMD --> GMU
    GMU --> SM0
    GMU --> SM1
    GMU --> SM2
    
    style HOST fill:#dbeafe,stroke:#3b82f6
    style DRIVER fill:#fef3c7,stroke:#f59e0b
    style GPU fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.8: Kernel launch execution flow from CPU to GPU</div>
        </div>

        <h3>Resource Constraints</h3>
        <p>
            Each SM has limited resources. The number of concurrent blocks depends on:
        </p>
        <ul>
            <li><strong>Registers:</strong> If kernel uses 64 regs/thread and SM has 65536 registers ‚Üí max 1024 threads</li>
            <li><strong>Shared memory:</strong> If block uses 16KB and SM has 64KB ‚Üí max 4 blocks</li>
            <li><strong>Thread slots:</strong> SM might limit to 2048 threads max</li>
            <li><strong>Block slots:</strong> SM might support 16-32 blocks max</li>
        </ul>

        <div class="engineer-note">
            <strong>Occupancy:</strong> The ratio of active warps to maximum possible warps. Higher occupancy 
            = better latency hiding. Use <code>cudaOccupancyMaxActiveBlocksPerMultiprocessor()</code> to calculate.
        </div>

        <h2>3.7 Synchronization Primitives</h2>

        <h3>Block-Level: __syncthreads()</h3>
        <div class="code-block">
<pre>__global__ void reductionKernel(float *input, float *output) {
    __shared__ float shared[256];
    
    int tid = threadIdx.x;
    shared[tid] = input[blockIdx.x * blockDim.x + tid];
    
    __syncthreads();  // Wait for all threads to load
    
    // Reduction in shared memory
    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            shared[tid] += shared[tid + stride];
        }
        __syncthreads();  // Wait for each step
    }
    
    if (tid == 0) output[blockIdx.x] = shared[0];
}</pre>
        </div>

        <h3>Atomic Operations</h3>
        <div class="code-block">
<pre>__global__ void histogram(int *input, int *bins, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        int bin = input[i];
        atomicAdd(&bins[bin], 1);  // Race-free increment
    }
}</pre>
        </div>

        <h2>3.8 Practical Patterns</h2>

        <h3>Pattern 1: Parallel Reduction</h3>
        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph INPUT["Input (8 elements)"]
        direction LR
        A["1"] ~~~ B["2"] ~~~ C["3"] ~~~ D["4"] ~~~ E["5"] ~~~ F["6"] ~~~ G["7"] ~~~ H["8"]
    end
    
    subgraph STEP1["Step 1: Stride = 4"]
        direction LR
        S1A["1+5=6"] ~~~ S1B["2+6=8"] ~~~ S1C["3+7=10"] ~~~ S1D["4+8=12"]
    end
    
    subgraph STEP2["Step 2: Stride = 2"]
        direction LR
        S2A["6+10=16"] ~~~ S2B["8+12=20"]
    end
    
    subgraph STEP3["Step 3: Stride = 1"]
        RESULT["16+20=36"]
    end
    
    INPUT --> STEP1 --> STEP2 --> STEP3
    
    style RESULT fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.9: Parallel reduction pattern ‚Äî O(log n) steps</div>
        </div>

        <h3>Pattern 2: Matrix Multiplication</h3>
        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph NAIVE["Naive: Each thread reads entire row/column"]
        A["Matrix A"] --> T["Thread (i,j)"]
        B["Matrix B"] --> T
        T --> C["C[i,j]"]
    end
    
    subgraph TILED["Tiled: Threads share data via shared memory"]
        AT["A Tile"] --> SM["Shared Memory"]
        BT["B Tile"] --> SM
        SM --> TC["All threads in block"]
        TC --> CT["C Tile"]
    end
    
    style TILED fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 3.10: Naive vs tiled matrix multiplication</div>
        </div>

        <h2>3.9 Chapter Summary</h2>

        <ul>
            <li><strong>SIMD:</strong> Single instruction operates on multiple data elements</li>
            <li><strong>SIMT:</strong> NVIDIA's innovation ‚Äî threads in software, SIMD in hardware</li>
            <li><strong>Thread hierarchy:</strong> Threads ‚Üí Warps ‚Üí Blocks ‚Üí Grids</li>
            <li><strong>Divergence:</strong> When threads in a warp take different paths, execution serializes</li>
            <li><strong>Programming models:</strong> CUDA, OpenCL, HIP</li>
            <li><strong>Synchronization:</strong> __syncthreads(), atomics, cooperative groups</li>
            <li><strong>Patterns:</strong> Reduction, matrix multiply, scan</li>
        </ul>

        <div class="key-takeaway">
            <strong>Next Steps:</strong> In Chapter 4, we'll dive into the actual hardware components that 
            implement these execution models ‚Äî ALUs, register files, schedulers, and load/store units.
        </div>

        <div class="exercise">
            <strong>Exercise 3.1:</strong> Write a CUDA kernel that multiplies every element of an array by 2. 
            How many blocks do you need for 1 million elements with 256 threads per block?
        </div>

        <div class="exercise">
            <strong>Exercise 3.2:</strong> Will this code diverge? Why?
            <code style="display: block; margin-top: 10px; padding: 10px; background: #f5f5f5;">if (threadIdx.x < 16) { expensiveComputation(); }</code>
        </div>

        <div class="exercise">
            <strong>Exercise 3.3:</strong> Modify the reduction kernel to find the maximum value instead of sum.
        </div>

        <div class="exercise">
            <strong>Exercise 3.4:</strong> Implement a 2D convolution kernel (3√ó3 filter). Each thread computes one output pixel.
        </div>

        <h2>Further Reading</h2>
        <ul>
            <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">NVIDIA CUDA Guide: SIMT Architecture</a></li>
            <li><a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">NVIDIA Blog: Warp-Level Primitives</a></li>
            <li>"Programming Massively Parallel Processors" Ch. 4: Thread Execution</li>
            <li>Paper: "Demystifying GPU Microarchitecture through Microbenchmarking"</li>
        </ul>

        <div class="chapter-nav">
            <a href="chapter-02.html">‚Üê Previous: GPU vs CPU</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-04.html">Next: Core Components ‚Üí</a>
        </div>
    </div>

    <script>
        mermaid.initialize({ 
            startOnLoad: true, 
            theme: 'base',
            themeVariables: {
                primaryColor: '#6366f1',
                primaryTextColor: '#1e293b',
                primaryBorderColor: '#4f46e5',
                lineColor: '#64748b',
                secondaryColor: '#10b981',
                tertiaryColor: '#f1f5f9'
            }
        });
    </script>
    <script src="../navigation.js"></script>
</body>
</html>
