<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 4: Core GPU Components | Create Your Own GPU</title>
    <link rel="stylesheet" href="../styles.css" />
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
    <style>
        .chapter-content { max-width: 800px; margin: 0 auto; padding: 40px 20px; }
        .chapter-nav { display: flex; justify-content: space-between; margin: 40px 0; padding: 20px; background: var(--panel); border-radius: var(--radius); }
        .figure { margin: 30px 0; padding: 20px; background: #f9f9ff; border-radius: 12px; }
        .figure-caption { font-style: italic; color: var(--muted); margin-top: 10px; text-align: center; }
        .key-takeaway { background: #e8f4fd; border-left: 4px solid #2196f3; padding: 16px; margin: 24px 0; }
        .exercise { background: #fff3e0; border-left: 4px solid #ff9800; padding: 16px; margin: 24px 0; }
        .engineer-note { background: #f3e5f5; border-left: 4px solid #9c27b0; padding: 16px; margin: 24px 0; }
        .warning-box { background: #fff3cd; border-left: 4px solid #ffc107; padding: 16px; margin: 24px 0; }
        .code-block { background: #1e1e1e; color: #d4d4d4; padding: 20px; border-radius: 8px; overflow-x: auto; margin: 20px 0; font-family: 'Consolas', 'Monaco', monospace; font-size: 14px; }
        .mermaid { background: transparent; text-align: center; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background: #f1f5f9; }
    </style>
</head>
<body>
    <div class="chapter-content">
        <div class="chapter-nav">
            <a href="chapter-03.html">← Previous: Parallel Execution</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-05.html">Next: ISA Design →</a>
        </div>

        <h1>Chapter 4: Core GPU Components</h1>
        
        <div class="meta" style="color: var(--muted); margin: 20px 0;">
            <span>Part I: GPU Fundamentals</span> • 
            <span>Reading time: ~60 min</span>
        </div>

        <h2>Introduction</h2>
        <p>
            Now that we understand <em>what</em> GPUs do (parallel execution with SIMT) and <em>why</em> 
            (throughput over latency), it's time to look at <em>how</em> — the actual hardware components 
            that make a GPU work.
        </p>

        <div class="key-takeaway">
            <strong>Bridge to Implementation:</strong> This chapter is conceptual but grounded in real hardware. 
            When we get to Part IV (RTL implementation), you'll write Verilog code for every component discussed here.
        </div>

        <h2>4.1 The Big Picture: GPU Core Architecture</h2>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph CORE["GPU Core (SM / CU)"]
        direction TB
        
        subgraph FRONTEND["Frontend"]
            IF["Instruction<br/>Fetch"] --> DEC["Decoder"]
            DEC --> SCHED["Warp<br/>Scheduler"]
        end
        
        subgraph EXECUTION["Execution Units"]
            direction LR
            ALU1["ALU<br/>Array"]
            ALU2["ALU<br/>Array"]
            LSU["Load/Store<br/>Unit"]
            SFU["Special<br/>Function"]
        end
        
        subgraph STORAGE["Storage"]
            RF["Register File<br/>(65K registers)"]
            SMEM["Shared Memory<br/>(64-128KB)"]
            L1["L1 Cache"]
        end
        
        SCHED --> EXECUTION
        EXECUTION <--> RF
        EXECUTION <--> SMEM
        LSU <--> L1
        
        L1 --> L2IF["L2 Interface"]
    end
    
    L2IF --> L2["L2 Cache / Global Memory"]
    
    style CORE fill:#f1f5f9,stroke:#64748b
    style FRONTEND fill:#dbeafe,stroke:#3b82f6
    style EXECUTION fill:#dcfce7,stroke:#22c55e
    style STORAGE fill:#fef3c7,stroke:#f59e0b
            </div>
            <div class="figure-caption">Figure 4.1: Simplified GPU core architecture block diagram</div>
        </div>

        <h2>4.2 Arithmetic Logic Unit (ALU)</h2>

        <h3>What Does an ALU Do?</h3>
        <ul>
            <li><strong>Arithmetic:</strong> ADD, SUB, MUL, DIV</li>
            <li><strong>Logical:</strong> AND, OR, XOR, NOT</li>
            <li><strong>Comparison:</strong> SLT (set less than), SEQ (set equal)</li>
            <li><strong>Bitwise:</strong> Shifts (SLL, SRL, SRA)</li>
        </ul>

        <h3>ALU Types</h3>
        <ul>
            <li><strong>INT32 ALU:</strong> 32-bit integer operations (1-cycle latency)</li>
            <li><strong>FP32 ALU:</strong> Single-precision floating-point (4-6 cycles)</li>
            <li><strong>FP64 ALU:</strong> Double-precision (8-16 cycles, fewer units)</li>
            <li><strong>FP16/BF16 ALU:</strong> Half-precision for AI (2× throughput)</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph ALU["FP32 ALU Pipeline"]
        direction TB
        
        A["Input A<br/>[31:0]"] --> UNPACK_A["Unpack<br/>(sign, exp, mantissa)"]
        B["Input B<br/>[31:0]"] --> UNPACK_B["Unpack<br/>(sign, exp, mantissa)"]
        
        UNPACK_A --> OP["Operation Logic<br/>(ADD/MUL/FMA)"]
        UNPACK_B --> OP
        OPCODE["Opcode"] --> OP
        
        OP --> NORM["Normalizer<br/>(round result)"]
        NORM --> PACK["Packer<br/>(reassemble)"]
        PACK --> RESULT["Result<br/>[31:0]"]
    end
    
    style ALU fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 4.2: Floating-point ALU pipeline stages</div>
        </div>

        <h3>ALU Design Trade-offs</h3>
        <table>
            <thead>
                <tr>
                    <th>Design Choice</th>
                    <th>Pros</th>
                    <th>Cons</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Many simple ALUs</td>
                    <td>High throughput, good for SIMT</td>
                    <td>Large area, high power</td>
                </tr>
                <tr>
                    <td>Pipelined ALUs</td>
                    <td>Higher clock frequency</td>
                    <td>Increased latency</td>
                </tr>
                <tr>
                    <td>Fused multiply-add (FMA)</td>
                    <td>2× throughput for A*B+C</td>
                    <td>More complex, larger area</td>
                </tr>
            </tbody>
        </table>

        <div class="engineer-note">
            <strong>Example:</strong> NVIDIA's RTX 4090 (gaming) has FP64 throughput 1/64th of FP32. 
            The H100 (datacenter) has 1/2 ratio. Same architecture, different ALU ratios!
        </div>

        <div class="code-block">
<pre>// Simplified 32-bit integer ALU
module alu_int32 (
    input  [31:0] a,
    input  [31:0] b,
    input  [3:0]  opcode,
    output reg [31:0] result,
    output reg zero
);
    always @(*) begin
        case (opcode)
            4'b0000: result = a + b;           // ADD
            4'b0001: result = a - b;           // SUB
            4'b0010: result = a & b;           // AND
            4'b0011: result = a | b;           // OR
            4'b0100: result = a ^ b;           // XOR
            4'b0101: result = a << b[4:0];     // SLL
            4'b0110: result = a >> b[4:0];     // SRL
            4'b0111: result = (a < b) ? 1 : 0; // SLT
            default: result = 32'h0;
        endcase
        zero = (result == 32'h0);
    end
endmodule</pre>
        </div>

        <h2>4.3 Register File</h2>

        <h3>Why Registers Matter</h3>
        <p>
            Registers are the <strong>fastest storage</strong> in a GPU. GPUs have <strong>massive</strong> register files:
        </p>
        <ul>
            <li>NVIDIA A100: 256 KB per SM (65,536 × 32-bit registers)</li>
            <li>AMD MI250X: 512 KB per CU</li>
            <li>Each thread uses 16-64 registers</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph RF["Register File (65,536 registers)"]
        direction TB
        
        subgraph W0["Warp 0 (768 regs)"]
            T0["Thread 0<br/>R0-R23"]
            T1["Thread 1<br/>R0-R23"]
            TN["...<br/>Thread 31"]
        end
        
        subgraph W1["Warp 1 (768 regs)"]
            T32["Thread 32<br/>R0-R23"]
        end
        
        subgraph W2["Warp 2...N"]
            MORE["More warps..."]
        end
    end
    
    RF --> NOTE["32 threads × 24 regs = 768 regs per warp"]
    
    style RF fill:#fef3c7,stroke:#f59e0b
    style W0 fill:#dcfce7,stroke:#22c55e
    style W1 fill:#dbeafe,stroke:#3b82f6
            </div>
            <div class="figure-caption">Figure 4.3: Register file partitioning among warps</div>
        </div>

        <div class="warning-box">
            <strong>Occupancy Limiter:</strong> Register usage often limits occupancy. 
            If your kernel uses 64 registers/thread and the SM has 65,536 registers, 
            you can only run 1,024 threads (32 warps) — far below the maximum of 2,048.
        </div>

        <h2>4.4 Program Counter (PC) and Instruction Fetch</h2>

        <p>
            The <strong>Program Counter</strong> tracks which instruction to execute next:
        </p>
        <ul>
            <li>Each <strong>warp</strong> has its own PC (not each thread!)</li>
            <li>All threads in a warp execute the same instruction (SIMT)</li>
            <li>PC increments by 1 each cycle, or jumps on branches</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph FETCH["Instruction Fetch Pipeline"]
        SCHED["Warp Scheduler<br/>(selects warp)"]
        
        subgraph PCS["PC Registers"]
            PC0["Warp 0: 0x40"]
            PC1["Warp 1: 0x38"]
            PC2["Warp 2: 0x44"]
        end
        
        ICACHE["Instruction Cache"]
        DEC["Decoder"]
        EXEC["Execute Units"]
        
        SCHED --> PCS
        PCS --> ICACHE
        ICACHE --> DEC
        DEC --> EXEC
    end
    
    BRANCH["Branch Logic"] -.->|Update PC| PCS
    
    style FETCH fill:#dbeafe,stroke:#3b82f6
            </div>
            <div class="figure-caption">Figure 4.4: PC and instruction fetch pipeline</div>
        </div>

        <h2>4.5 Warp Scheduler</h2>

        <p>
            The <strong>warp scheduler</strong> is the GPU's traffic controller. Every cycle, it decides:
        </p>
        <ul>
            <li>Which warp(s) to execute</li>
            <li>Which execution unit to use</li>
            <li>Whether to issue or stall</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
stateDiagram-v2
    [*] --> IDLE
    
    IDLE --> SELECT: Warp ready
    SELECT --> ISSUE: Found ready warp
    ISSUE --> WAIT: Instruction issued
    WAIT --> IDLE: Pipeline clear
    
    IDLE --> IDLE: No ready warps
    
    note right of SELECT: Priority encoder<br/>finds first ready warp
    note right of ISSUE: Enable signal<br/>to execution units
            </div>
            <div class="figure-caption">Figure 4.5: Warp scheduler state machine</div>
        </div>

        <h3>Scheduling Policies</h3>
        <ol>
            <li><strong>Round-robin:</strong> Cycle through warps in order (simple, fair)</li>
            <li><strong>Greedy-then-oldest (GTO):</strong> Issue same warp until stall, then oldest ready</li>
            <li><strong>Two-level:</strong> Prioritize warps with guaranteed data</li>
        </ol>

        <h3>Scoreboarding</h3>
        <p>
            The scheduler uses a <strong>scoreboard</strong> to track dependencies:
        </p>
        <ul>
            <li>Each register has a "pending write" bit</li>
            <li>Check if source registers are ready before issuing</li>
            <li>Mark warp as stalled if dependency exists</li>
        </ul>

        <h2>4.6 Load/Store Unit (LSU)</h2>

        <p>
            The LSU handles all memory operations:
        </p>
        <ul>
            <li>Loads: Read from global memory, shared memory, cache</li>
            <li>Stores: Write to global/shared memory</li>
            <li>Atomics: Read-modify-write operations</li>
        </ul>

        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph LSU["Load/Store Unit Pipeline"]
        direction TB
        
        ADDR["Address Generation<br/>(base + offset)"]
        COAL["Coalescing Unit<br/>(merge accesses)"]
        CACHE["Cache Lookup<br/>(L1 check)"]
        WAIT["Wait for Data<br/>(scheduler switches)"]
        WB["Writeback<br/>(to registers)"]
        
        ADDR --> COAL --> CACHE --> WAIT --> WB
    end
    
    style LSU fill:#dcfce7,stroke:#22c55e
            </div>
            <div class="figure-caption">Figure 4.6: LSU pipeline stages</div>
        </div>

        <h3>Memory Coalescing</h3>
        <div class="figure">
            <div class="mermaid">
flowchart LR
    subgraph GOOD["Good: Coalesced Access"]
        direction TB
        G0["Thread 0: A[0]"]
        G1["Thread 1: A[1]"]
        G2["Thread 2: A[2]"]
        GN["Thread 31: A[31]"]
        
        G0 & G1 & G2 & GN --> TRANS1["1 Transaction<br/>(128 bytes)"]
    end
    
    subgraph BAD["Bad: Strided Access"]
        direction TB
        B0["Thread 0: A[0]"]
        B1["Thread 1: A[128]"]
        B2["Thread 2: A[256]"]
        
        B0 --> T1["Transaction 1"]
        B1 --> T2["Transaction 2"]
        B2 --> T3["Transaction 3"]
    end
    
    style GOOD fill:#dcfce7,stroke:#22c55e
    style BAD fill:#fee2e2,stroke:#ef4444
            </div>
            <div class="figure-caption">Figure 4.7: Coalesced (1 transaction) vs strided (32 transactions)</div>
        </div>

        <h2>4.7 Execution Pipeline and Datapath</h2>

        <h3>Pipeline Stages</h3>
        <ol>
            <li><strong>Fetch (F):</strong> Read instruction from I-cache</li>
            <li><strong>Decode (D):</strong> Decode opcode, identify registers</li>
            <li><strong>Issue (I):</strong> Check scoreboard, schedule warp</li>
            <li><strong>Read (R):</strong> Read operands from register file</li>
            <li><strong>Execute (E):</strong> ALU/LSU/SFU operation</li>
            <li><strong>Memory (M):</strong> Cache access (for loads/stores)</li>
            <li><strong>Writeback (W):</strong> Write result to register file</li>
        </ol>

        <div class="figure">
            <div class="mermaid">
gantt
    title GPU Pipeline - Multiple Warps
    dateFormat X
    axisFormat %s
    
    section Warp 0
    Fetch   :a1, 0, 1
    Decode  :a2, 1, 2
    Issue   :a3, 2, 3
    Read    :a4, 3, 4
    Execute :a5, 4, 5
    Memory  :a6, 5, 6
    Write   :a7, 6, 7
    
    section Warp 1
    Fetch   :b1, 1, 2
    Decode  :b2, 2, 3
    Issue   :b3, 3, 4
    Read    :b4, 4, 5
    Execute :b5, 5, 6
    Memory  :b6, 6, 7
    Write   :b7, 7, 8
    
    section Warp 2
    Fetch   :c1, 2, 3
    Decode  :c2, 3, 4
    Issue   :c3, 4, 5
    Read    :c4, 5, 6
    Execute :c5, 6, 7
    Memory  :c6, 7, 8
    Write   :c7, 8, 9
            </div>
            <div class="figure-caption">Figure 4.8: Pipelined execution across multiple warps</div>
        </div>

        <h3>Datapath Overview</h3>
        <div class="figure">
            <div class="mermaid">
flowchart TB
    subgraph DATAPATH["GPU Core Datapath"]
        PC["Program<br/>Counter"]
        ICACHE["Instruction<br/>Cache"]
        DEC["Decoder"]
        SCHED["Warp<br/>Scheduler"]
        
        RF["Register File<br/>(65K regs)"]
        
        ALU["ALU<br/>(32-wide)"]
        
        BRANCH["Branch<br/>Logic"]
        
        PC --> ICACHE --> DEC --> SCHED
        SCHED --> RF
        RF --> |"Src A, Src B"| ALU
        ALU --> |"Result"| RF
        
        BRANCH -.-> |"Update PC"| PC
        ALU --> BRANCH
    end
    
    style DATAPATH fill:#f1f5f9,stroke:#64748b
            </div>
            <div class="figure-caption">Figure 4.9: Simplified datapath showing data flow</div>
        </div>

        <h2>4.8 Control Signals and FSMs</h2>

        <p>Every hardware component has two parts:</p>
        <ul>
            <li><strong>Datapath:</strong> ALUs, registers, wires that move data</li>
            <li><strong>Control:</strong> FSMs, decoders, enable signals</li>
        </ul>

        <div class="code-block">
<pre>// Instruction decoder generates control signals
module decoder (
    input [15:0] instruction,
    output reg alu_enable,
    output reg lsu_enable,
    output reg [3:0] alu_op,
    output reg mem_read,
    output reg mem_write,
    output reg reg_write
);
    wire [3:0] opcode = instruction[15:12];
    
    always @(*) begin
        {alu_enable, lsu_enable, mem_read, mem_write, reg_write} = 5'b0;
        
        case (opcode)
            4'b0000: begin  // ADD
                alu_enable = 1; alu_op = 4'b0000; reg_write = 1;
            end
            4'b0001: begin  // SUB
                alu_enable = 1; alu_op = 4'b0001; reg_write = 1;
            end
            4'b1000: begin  // LOAD
                lsu_enable = 1; mem_read = 1; reg_write = 1;
            end
            4'b1001: begin  // STORE
                lsu_enable = 1; mem_write = 1;
            end
        endcase
    end
endmodule</pre>
        </div>

        <h2>4.9 Putting It All Together</h2>

        <h3>Execution Flow Example</h3>
        <div class="code-block">
<pre>// Kernel: c[i] = a[i] + b[i]
// Assembly:
LOAD  R1, a[threadIdx]   // R1 = a[i]
LOAD  R2, b[threadIdx]   // R2 = b[i]
ADD   R3, R1, R2         // R3 = R1 + R2
STORE c[threadIdx], R3   // c[i] = R3</pre>
        </div>

        <div class="figure">
            <div class="mermaid">
sequenceDiagram
    participant S as Scheduler
    participant LSU as Load/Store Unit
    participant MEM as Memory
    participant ALU as ALU
    participant RF as Registers
    
    Note over S: Warp 0: LOAD R1, a[tid]
    S->>LSU: Issue load
    LSU->>MEM: Memory request
    Note over S: Warp 0 stalls,<br/>switch to Warp 1
    
    MEM-->>LSU: Data ready (200 cycles)
    LSU->>RF: Write R1
    
    Note over S: Warp 0: ADD R3, R1, R2
    S->>RF: Read R1, R2
    RF->>ALU: Operands
    ALU->>RF: Write R3
    Note over ALU: 1 cycle!
            </div>
            <div class="figure-caption">Figure 4.10: Execution trace showing latency hiding</div>
        </div>

        <h3>Key Insights</h3>
        <ul>
            <li><strong>Memory dominates:</strong> 200+ cycles vs 1 cycle for ALU</li>
            <li><strong>Latency hiding:</strong> Scheduler issues other warps while waiting</li>
            <li><strong>SIMD parallelism:</strong> 32 ALUs execute simultaneously</li>
        </ul>

        <h2>4.10 Chapter Summary</h2>

        <ul>
            <li><strong>ALU:</strong> Arithmetic engine, FP32/FP64/INT variants, pipelined</li>
            <li><strong>Register File:</strong> 65K+ registers, multi-ported, dynamically allocated</li>
            <li><strong>Program Counter:</strong> Per-warp PC, branch handling</li>
            <li><strong>Warp Scheduler:</strong> Traffic controller, scoreboarding</li>
            <li><strong>Load/Store Unit:</strong> Memory access, coalescing, cache interface</li>
            <li><strong>Pipeline:</strong> Fetch → Decode → Issue → Read → Execute → Memory → Writeback</li>
        </ul>

        <div class="key-takeaway">
            <strong>Next Steps:</strong> In Chapter 5, we'll design a complete Instruction Set Architecture (ISA) 
            for our GPU, defining instruction formats, encoding, and semantics.
        </div>

        <div class="exercise">
            <strong>Exercise 4.1:</strong> Calculate register file size: 32 warps × 32 threads × 24 registers = ?
        </div>

        <div class="exercise">
            <strong>Exercise 4.2:</strong> Design a 2-operand ALU supporting ADD, SUB, AND, OR, XOR.
        </div>

        <div class="exercise">
            <strong>Exercise 4.3:</strong> How many 128-byte transactions for:
            <ul style="margin-top: 10px;">
                <li>Pattern A: Thread i accesses A[i]</li>
                <li>Pattern B: Thread i accesses A[i * 32]</li>
            </ul>
        </div>

        <div class="exercise">
            <strong>Exercise 4.4:</strong> Implement a round-robin scheduler for 8 warps in Verilog.
        </div>

        <h2>Further Reading</h2>
        <ul>
            <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation">NVIDIA CUDA Guide: Hardware Implementation</a></li>
            <li>"Computer Architecture: A Quantitative Approach" — GPU chapters</li>
            <li><a href="https://github.com/adam-maj/tiny-gpu">tiny-gpu repository</a> — See src/alu.sv, src/registers.sv</li>
        </ul>

        <div class="chapter-nav">
            <a href="chapter-03.html">← Previous: Parallel Execution</a>
            <a href="../table-of-contents.html">Table of Contents</a>
            <a href="chapter-05.html">Next: ISA Design →</a>
        </div>
    </div>

    <script>
        mermaid.initialize({ 
            startOnLoad: true, 
            theme: 'base',
            themeVariables: {
                primaryColor: '#6366f1',
                primaryTextColor: '#1e293b',
                primaryBorderColor: '#4f46e5',
                lineColor: '#64748b',
                secondaryColor: '#10b981',
                tertiaryColor: '#f1f5f9'
            }
        });
    </script>
    <script src="../navigation.js"></script>
</body>
</html>
